\documentclass[11pt,letterpaper]{article}

%\usepackage{showlabels}
\usepackage{fullpage}
\usepackage{pslatex}
%\usepackage{latexsym}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{url}
%\usepackage[colorinlistoftodos]{todonotes}
\usepackage{rotating}
\usepackage{natbib}
\usepackage{amssymb}
\usepackage{lingmacros}

\usepackage{tikz-dependency}
\usepackage{longtable}

\usepackage{changepage}


%\usepackage{paralist} 
%\usepackage{graphicx} 
%\usepackage{multirow} 
%\usepackage{enumitem}
\usepackage{linguex}
%\raggedbottom

\definecolor{Purple}{RGB}{255,10,140}
\newcommand{\jd}[1]{\textcolor{Purple}{[jd: #1]}} 


\newcommand{\R}[0]{\mathbb{R}}
\newcommand{\E}[0]{\mathbb{E}}
\newcommand{\Ff}[0]{\mathcal{F}}

\usepackage{multirow}

\newcommand{\soft}[1]{}
\newcommand{\nopreview}[1]{}
\newcommand\comment[1]{{\color{red}#1}}
\newcommand\mhahn[1]{{\color{red}(mhahn #1)}}
\newcommand\jdegen[1]{{\color{red}(jdegen #1)}}
\newcommand\note[1]{{\color{red}(#1)}}
\newcommand\REF[0]{{\color{red}(REF) }}
\newcommand\CITE[0]{{\color{red}(CITE) }}
\newcommand\rljf[1]{{\color{red}(#1)}}
\newcommand{\key}[1]{\textbf{#1}}
\newcommand\revision[1]{{#1}}
\newcommand\revdeleted[1]{{}}


%\usepackage[dvipsnames]{xcolor}

\usepackage{amsthm}

\newcommand{\thetad}[0]{{\theta_d}}
\newcommand{\thetal}[0]{{\theta_{LM}}}

\newcounter{theorem}
\newcounter{def}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{thm}[theorem]{Theorem}
\newtheorem{definition}[def]{Definition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{question}[theorem]{Question}
\newtheorem{example}[theorem]{Example}
\newtheorem{lemma}[theorem]{Lemma}


\frenchspacing
%\def\baselinestretch{0.975}

%\emnlpfinalcopy
%\def\emnlppaperid{496}

\title{Modeling word and morpheme order in natural language as an efficient tradeoff of memory and surprisal}
\author{\begin{tabular}{ccc}Michael Hahn & Judith Degen & Richard Futrell \\ Stanford University & Stanford University & UC Irvine \\ mhahn2@stanford.edu & jdegen@stanford.edu & rfutrell@uci.edu \end{tabular}}
\date{}

\begin{document}

\maketitle


\begin{abstract}
Memory limitations are known to constrain language comprehension and production, and have been argued to account for crosslinguistic word order regularities.
However, a systematic assessment of the role of memory limitations in language structure has proven elusive, in part because it is hard to extract precise large-scale quantitative generalizations about language from existing mechanistic models of memory use in sentence processing.
We provide an architecture-independent information-theoretic formalization of memory limitations which enables a simple calculation of the memory efficiency of languages.
Our notion of memory efficiency is based on the idea of a \emph{memory--surprisal tradeoff}: a certain level of average surprisal per word can only be achieved at the cost of storing some amount of information about past context.
Based on this notion of memory usage, we advance the \emph{Efficient Tradeoff Hypothesis}: the order of elements in natural language is under pressure to enable favorable memory-surprisal tradeoffs.
We derive that languages enable more efficient tradeoffs when they exhibit \emph{information locality}: when predictive information about an element is concentrated in its recent past.
We provide empirical evidence from three test domains in support of the Efficient Tradeoff Hypothesis:
a reanalysis of a miniature artificial language learning experiment, a large-scale study of word order in corpora of 54 languages, and an analysis of morpheme order in two agglutinative languages. These results suggest that principles of order in natural language can be explained via highly generic cognitively motivated principles and lend support to efficiency-based models of the structure of human language.
\end{abstract}

%\jd{2 stylistic notes on plots: 1. it would generally be better to use a color-blind friendly palette. 2. it would be helpful if a particular color had a particular semantics throughout the paper, eg, if real languages across plots always had the same color, if different baselines that correspond to analogous versions of each other across plots did, etc. not strictly necessary, just makes it a lot easier on the reader.}
 
\section{Introduction}
\footnotetext{Â© 2021, American Psychological Association. This article may not exactly replicate the authoritative document published in the APA journal. It is not the copy of record. The copy of record is available at \url{https://doi.apa.org/doi/10.1037/rev0000269} \\ All code and data are freely available at \url{https://github.com/m-hahn/memory-surprisal} . The SI appendix is available at \url{https://doi.org/10.1037/rev0000269.supp} .}

\input{introduction.tex}


\section{Background}\label{sec:background}


\input{background.tex}


\section{Memory-Surprisal Tradeoff}\label{sec:ms-tradeoff}

\input{tradeoff.tex}

\section{Study 1: Memory and Dependency Length}\label{sec:toy-study}

\input{toy-experiment.tex}



\section{Study 2: Large-Scale Evidence that Word Orders Optimize Memory-Surprisal Tradeoff}
\label{sec:main-experiment}

\input{main-experiment.tex}

\subsection{Controlling for Information Structure}\label{subsec:freedom}

\input{freedom-control.tex}

\section{Study 3: Morpheme Order}\label{sec:morphemes}

\input{word-internal.tex}


\section{General Discussion}\label{sec:discussion}

\input{discussion.tex}


\section{Conclusion}\label{sec:conclusion}

In this work, we have provided evidence that human languages order elements in a way that reduces cognitive resource requirements, in particular memory effort.
We provided an information-theoretic formalization of memory requirements as a tradeoff of memory and surprisal.
We showed theoretically that languages have more efficient tradeoffs when they show stronger degrees of information locality.
Information locality provides a formalization of various locality principles from the linguistic literature, including dependency locality \citep{gibson1998linguistic}, domain minimization \citep{hawkins2004efficiency}, and the proximity principle \citep{givon1985iconicity}.
Using this result, we provided evidence that languages order words and morphemes in such a way as to provide efficient memory--surprisal tradeoffs.
Therefore, the memory--surprisal tradeoff simultaneously provides (1) a unified explanation of diverse typological phenomena which is rigorously grounded in the psycholinguistics literature, (2) a theory which makes new successful quantitative predictions about word and morpheme order within and across languages, and (3) a mathematical framework relating universals of language to principles of efficient coding from information theory. 

Our result shows that wide-ranging principles of order in natural language can be explained from highly generic cognitively-motivated information-theoretic principles. The locality properties we have discussed are some of the most characteristic properties of natural language, setting natural language apart from other codes studied in information theory.
Therefore, our result raises the question of whether other distinctive characteristics of language---for example, mildly context-sensitive syntax, duality of patterning, and compositionality---might also be explained in terms of information-theoretic resource constraints.



\bibliographystyle{apalike}
\bibliography{literature}

\end{document}






