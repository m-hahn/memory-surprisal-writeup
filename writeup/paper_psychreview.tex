%%
%% This is file `./samples/longsample.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% apa7.dtx  (with options: `longsample')
%% ----------------------------------------------------------------------
%% 
%% apa7 - A LaTeX class for formatting documents in compliance with the
%% American Psychological Association's Publication Manual, 7th edition
%% 
%% Copyright (C) 2019 by Daniel A. Weiss <daniel.weiss.led at gmail.com>
%% 
%% This work may be distributed and/or modified under the
%% conditions of the LaTeX Project Public License (LPPL), either
%% version 1.3c of this license or (at your option) any later
%% version.  The latest version of this license is in the file:
%% 
%% http://www.latex-project.org/lppl.txt
%% 
%% Users may freely modify these files without permission, as long as the
%% copyright line and this statement are maintained intact.
%% 
%% This work is not endorsed by, affiliated with, or probably even known
%% by, the American Psychological Association.
%% 
%% ----------------------------------------------------------------------
%% 
\documentclass[man]{apa7}

\usepackage{lipsum}

\usepackage[american]{babel}


\usepackage{amsmath}
\usepackage{lingmacros}

\usepackage{tikz-dependency}

\usepackage{changepage}

\usepackage{linguex}


\usepackage{csquotes}
\usepackage[bibencoding=utf8, style=apa,sortcites=true,sorting=nyt,backend=biber]{biblatex}
%\usepackage[backend=biber, bibencoding=utf8, style=authoryear, citestyle=authoryear]{biblatex} % https://tex.stackexchange.com/questions/402714/biblatex-not-working-with-biber-2-8-error-reading-bib-file-in-ascii-format
\DeclareLanguageMapping{american}{american-apa}
\addbibresource{literature.bib}



\title{Modeling word and morpheme order in natural language as an efficient tradeoff of memory and surprisal}
\shorttitle{Tradeoff of Memory and Surprisal}

\author{Michael Hahn$^1$, Judith Degen$^1$, Richard Futrell$^2$}
\affiliation{$^1$ Stanford University, $^2$ University of California, Irvine}

\leftheader{Hahn, Degen, Futrell}


%\newcommand\citep[1]{\parencite{#1}}
%\newcommand\citet[1]{\Textcite{#1}}

\newcommand{\citep}{\parencite}
\newcommand{\citet}{\Textcite}

\definecolor{Purple}{RGB}{255,10,140}
\newcommand{\jd}[1]{\textcolor{Purple}{[jd: #1]}} 


\newcommand{\soft}[1]{}
\newcommand{\nopreview}[1]{}
\newcommand\comment[1]{{\color{red}#1}}
\newcommand\mhahn[1]{{\color{red}(mhahn #1)}}
\newcommand\jdegen[1]{{\color{red}(jdegen #1)}}
\newcommand\REF[0]{{\color{red}(REF) }}
\newcommand\CITE[0]{{\color{red}(CITE) }}
\newcommand\rljf[1]{{\color{red}(#1)}}
\newcommand{\key}[1]{\textbf{#1}}


%\usepackage[dvipsnames]{xcolor}

\usepackage{amsthm}

\newcommand{\thetad}[0]{{\theta_d}}
\newcommand{\thetal}[0]{{\theta_{LM}}}

\newcounter{theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{thm}[theorem]{Theorem}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{question}[theorem]{Question}
\newtheorem{example}[theorem]{Example}
\newtheorem{lemma}[theorem]{Lemma}

\abstract{Online memory limitations are well-established as a factor impacting sentence processing and have been argued to account for crosslinguistic word order regularities. Building off expectation-based models of language processing, we provide an information-theoretic formalization of these memory limitations. We introduce the idea of a memory--surprisal tradeoff: comprehenders can achieve lower average surprisal per word at the cost of storing more information about past context. We show that the shape of the tradeoff is determined in part by word order. In particular, languages will enable more efficient tradeoffs when they exhibit information locality: when predictive information about a word is concentrated in the word's recent past. We show evidence from corpora of 54 real languages showing that languages allow for more efficient memory--surprisal tradeoffs than random baseline word order grammars. 
We then use data from two agglutinative languages to show that morpheme order also optimizes efficiency of this tradeoff.
}

\keywords{Language universals, sentence processing, information theory}

\authornote{
   \addORCIDlink{Michael Hahn}{https://orcid.org/0000-0003-4828-4834}

  Correspondence concerning this article should be addressed to Michael Hahn, Department of Linguistics, Stanford University Stanford, CA 94305-2150.  E-mail: mhahn2@stanford.edu}

\begin{document}

\maketitle


%\Textcite{vonDavier2011} said this,
%too \parencite{vonDavier2011,Lassen2006}.  Further evidence comes from
%other sources \parencite{Shotton1989,Lassen2006}.  \lipsum[3]
\section{Introduction}
\input{introduction.tex}


\section{Background}\label{sec:background}


\input{background.tex}


\section{Memory-Surprisal Tradeoff}\label{sec:ms-tradeoff}

\input{tradeoff.tex}

\section{Study 1: Memory and Dependency Length}\label{sec:toy-study}

\input{toy-experiment.tex}



\section{Study 2: Large-Scale Evidence that Word Orders Optimize Memory-Surprisal Tradeoff}
\label{sec:main-experiment}

\input{main-experiment.tex}

\subsection{Controlling for Information Structure}\label{subsec:freedom}

\input{freedom-control.tex}

\section{Study 3: Morpheme Order}\label{sec:morphemes}

\input{word-internal.tex}


\section{General Discussion}\label{sec:discussion}

\input{discussion.tex}


\section{Conclusion}\label{sec:conclusion}

In this work, we have provided evidence that human languages order elements in a way that reduces cognitive resource requirements, in particular memory effort.
We provided an information-theoretic formalization of memory requirements as a tradeoff of memory and surprisal.
We show theoretically that languages have more efficient tradeoffs when they show stronger degrees of information locality.
Information locality provides a formalization of various locality principles from the linguistic literature, including dependency locality \citep{gibson1998linguistic}, domain minimization \citep{hawkins2004efficiency}, and the proximity principle \citep{givon1985iconicity}.
Using this result, we provided evidence that languages order words and morphemes in such a way as to provide efficient memory--surprisal tradeoffs.

Our result shows that wide-ranging principles of order in natural language can be explained from highly generic cognitively-motivated information-theoretic principles. The locality properties we have discussed are some of the most characteristic properties of natural language, setting natural language apart from other codes studied in information theory.
Therefore, our result raises the question of whether other distinctive characteristics of language---for example, mildly context-sensitive syntax, duality of patterning, and compositionality---might also be explained in terms of information-theoretic resource constraints on production and comprehension.






\printbibliography

\end{document}

\appendix

\section{Instrument}
\label{app:instrument}

As shown in Figure~\ref{fig:Figure2}, these results are impressive. \lipsum[20]

%\begin{figure}
%    \caption{This is my second figure caption.}
%    \includegraphics[bb=0in 0in 2.5in 2.5in, height=2.5in, width=2.5in]{Figure1.pdf}
%    \label{fig:Figure2}
%\end{figure}

\lipsum[21]
\section{Pilot Data}
\label{app:surveydata}

The detailed results are shown in Table~\ref{tab:DeckedTable}. \lipsum[22]

\begin{table}
  \begin{threeparttable}
    \caption{A More Complex Decked Table}
    \label{tab:DeckedTable}
    \begin{tabular}{@{}lrrr@{}}         \toprule
    Distribution type  & \multicolumn{2}{l}{Percentage of} & Total number   \\
                       & \multicolumn{2}{l}{targets with}  & of trials per  \\
                       & \multicolumn{2}{l}{segment in}    & participant    \\ \cmidrule(r){2-3}
                                    &  Onset  &  Coda            &          \\ \midrule
    Categorical -- onset\tabfnm{a}  &    100  &     0            &  196     \\
    Probabilistic                   &     80  &    20\tabfnm{*}  &  200     \\
    Categorical -- coda\tabfnm{b}   &      0  &   100\tabfnm{*}  &  196     \\ \midrule
    \end{tabular}
    \begin{tablenotes}[para,flushleft]
        {\small
            \textit{Note.} All data are approximate.

            \tabfnt{a}Categorical may be onset.
            \tabfnt{b}Categorical may also be coda.

            \tabfnt{*}\textit{p} < .05.
            \tabfnt{**}\textit{p} < .01.
         }
    \end{tablenotes}
  \end{threeparttable}
\end{table}

\lipsum[23]


%% 
%% Copyright (C) 2019 by Daniel A. Weiss <daniel.weiss.led at gmail.com>
%% 
%% This work may be distributed and/or modified under the
%% conditions of the LaTeX Project Public License (LPPL), either
%% version 1.3c of this license or (at your option) any later
%% version.  The latest version of this license is in the file:
%% 
%% http://www.latex-project.org/lppl.txt
%% 
%% Users may freely modify these files without permission, as long as the
%% copyright line and this statement are maintained intact.
%% 
%% This work is not endorsed by, affiliated with, or probably even known
%% by, the American Psychological Association.
%% 
%% This work is "maintained" (as per LPPL maintenance status) by
%% Daniel A. Weiss.
%% 
%% This work consists of the file  apa7.dtx
%% and the derived files           apa7.ins,
%%                                 apa7.cls,
%%                                 apa7.pdf,
%%                                 README,
%%                                 APA7american.txt,
%%                                 APA7british.txt,
%%                                 APA7dutch.txt,
%%                                 APA7english.txt,
%%                                 APA7german.txt,
%%                                 APA7ngerman.txt,
%%                                 APA7greek.txt,
%%                                 APA7czech.txt,
%%                                 APA7turkish.txt,
%%                                 APA7endfloat.cfg,
%%                                 Figure1.pdf,
%%                                 shortsample.tex,
%%                                 longsample.tex, and
%%                                 bibliography.bib.
%% 
%%
%% End of file `./samples/longsample.tex'.
