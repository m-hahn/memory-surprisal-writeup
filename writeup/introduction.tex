

%Each human language is a system for encoding an infinite variety of thoughts into sentences, i.e. linear strings of words.
%A primary goal of the field of linguistics is to characterize and explain the commonalities and differences among these systems, and so to gain insight into the cognitive machinery that underpins this fundamental aspect of human intelligence.
Natural language solves the problem of allowing humans to communicate effectively given the limited mental and physiological resources available for producing and understanding utterances. Here, we investigate whether human languages are grammatically structured in a way that reduces the cognitive resource requirements for production and comprehension. In particular, we study whether word orders are structured so as to reduce working memory resource requirements in language processing. We do so by introducing a new information-theoretic formalization of the relationship between word order and memory usage in language processing, which allows us to demonstrate that the memory resource requirements for real human language are less than would be expected from baselines.

The suggestion that the structure of human language reflects a need for efficient processing under resource limitations has been present in the theoretical and functional linguistics literature for decades~\citep{yngve1961,berwick1984grammatical,hawkins1994performance,jaeger2011language,gibson2019efficiency}. The idea has been summed up in \citeauthor{hawkins1994efficiency}'s (\citeyear{hawkins1994efficiency}) \key{Performance--Grammar Correspondence Hypothesis} (PGCH), which holds that grammars are structured so that the typical utterance is easy to produce and comprehend under performance constraints.
The idea has also been controversial, with prominent claims that efficiency in this sense has had no or only a minimal effect on the structure of human language \citep{chomsky2005three}.

One major source of resource limitation in language processing is working memory. The existence of this resource limitation is indicated by measurable processing difficulty that has been observed during incremental language comprehension at moments when it seems that an item must be retrieved from working memory \citep{gibson1998syntactic,gibson1999memory,gibson2000dependency,mcelree,lewis2005activationbased,bartek2011search,nicenboim2015working}. Typically, working memory constraints manifest as processing difficulty associated with long dependencies: that is, cases where two words which depend on each other for their meaning are separated by many intervening words. Because of this, working memory limitations have been held to create a pressure for \key{dependency locality} in word order: that is, words which depend on each other should be close to each other \citep[for recent reviews of this idea, see][]{liu-dependency-2017,temperley-minimizing-2018}.

Our contribution is to present a new, highly general formalization of the relationship between word order and working memory in language processing. We formalize the notion of memory constraints in language processing in terms of what we call the \key{memory--surprisal tradeoff}: the idea that it is possible to achieve greater ease of word-by-word comprehension, and greater accuracy in word-by-word production, at the cost of investing more computational resources into remembering previous words.
The shape of this tradeoff depends on the grammar of a language, and in particular its word order properties.
We characterize memory resources in a theory-neutral, information-theoretic manner based on the theory of lossy data compression \citep{cover2006elements,berger}. % cite the crutchfield nature review, and predictive info bottleneck


Within our framework, we prove a theorem showing that shorter dependencies coincide with lower memory requirements, and we demonstrate that word orders with short dependencies do indeed engender lower working memory resource requirements in toy languages from previous literature. Finally, we show that real word orders in corpora of TODO languages have lower memory requirements than would be expected under artificial baseline comparison grammars. 

Our work not only formalizes and tests an old idea in functional linguistics and psycholinguistics, it also opens up connections between those fields and the statistical analysis of natural language \citep{debowski-excess-2011,bentz2017word,lin-critical-2017}, and more broadly, between linguistics and fields that have studied information-processing costs and resource requirements in brains \citep{friston}, organisms \citep{england}, and physical systems from a thermodynamic perspective \citep{still2012thermodynamic}. %For example, our memory--surprisal tradeoff is a variant of the Predictive Rate--Distortion curve introduced by \citet{still-optimal-2010} and studied by \citet{marzen-nearly-2017}. 








%Complementing this work, a number of researchers in linguistic typology and corpus linguistics have shown how various universal properties of grammars can be explained in terms of memory-based processing difficulty \citep{hawkins2004efficiency,hawkins2014crosslinguistic,ferrericancho2006syntactic,gildea2010grammars,futrell2015largescale,liu2017dependency,temperley2018dependency}.
%We will see that one of the most successful theories in this domain, the principle of \key{dependency length minimization}, falls out of our formalization of processing efficiency as a special case.
%Our work formalizes and generalizes this previous work.




%For each amount of memory invested, a language permits a certain level of ease of processing 
%The resulting tradeoff of memory and surprisal of depends on the word order properties of a language. Analogous results also hold for language production by resource-constrained speakers. We show evidence that the preferred word orders of natural languages are those that enable efficient memory--surprisal tradeoffs.



%These strategies can be seen as sets of rules that differ in the way ...
%Languages differ considerably in the rules they apply to order information: English orders the object after the verb, Japanese places it before the verb \note{make more explicit here}.
%Explaining the variation in the strategies languages use to  order underlying hierarchical structures into linear strings of words has been one of the foci of linguistic research (\jd{CITE}).
%We show that these strategies reflect an underlying optimization principle: human languages are adapted to limitations in human working memory.

%We test the hypothesis that human languages represent different solutions to the problem of efficient computation with constrained working memory.






%\mhahn{original intro}
%Since the 1950s, it has been a persistent suggestion that human language processing is shaped by a resource bottleneck in short-term memory.
%Language is produced and comprehended incrementally in a way that crucially requires both speaker and listener to use an active memory store to keep track of what was previously said.
%Since short-term memory of this kind is known to be highly limited in capacity \citep{miller1956magical}, it makes sense for these capacity limits to comprise a major constraint on production and comprehension.
%Indeed, a great deal of work in sentence processing has focused on characterizing the effects of memory constraints on language processing \citep{gibson-linguistic-1998,lewis-activation-based-2005,levy2013memory}.

%At the same time, the field of functional linguistics has argued that these resource constraints not only affect online language processing, they also shape the form of human language itself.
%For example, the Performace--Grammar Correspondence Hypothesis (PGCH) of \citet{hawkins1994performance} holds that forms which are practically easier to produce and comprehend end up becoming part of the grammars of languages, and that this process can explain several of the universal properties of human languages originally documented by \citet{greenberg-universals-1963}.




