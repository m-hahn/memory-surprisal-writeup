
Natural language is a powerful tool that allows humans to communicate, albeit under inherent cognitive resource limitations.
Here, we investigate whether human languages are grammatically structured in a way that reduces the cognitive resource requirements for comprehension, compared to counterfactual languages that differ in grammatical structure.

The suggestion that the structure of human language reflects a need for efficient processing under resource limitations has been present in the linguistics and cognitive science literature for decades~\citep{yngve1960model,berwick1984grammatical,hawkins1994performance,chomsky2005three,jaeger2011language,gibson2019efficiency,hahn2020universals}. The idea has been summed up in \citeauthor{hawkins2004efficiency}'s (\citeyear{hawkins2004efficiency}) \key{Performance--Grammar Correspondence Hypothesis} (PGCH), which holds that grammars are structured so that the typical utterance is easy to produce and comprehend under performance constraints.

One major source of resource limitation in language processing is incremental memory use. 
When producing and comprehending language in real time, a language user must keep track of what they have already produced or heard in some kind of incremental memory store, which is subject to resource constraints.
These memory constraints have been argued to underlie various \key{locality principles} which linguists have used to predict the orders of words within sentences and morphemes within words \citep[e.g.][]{behaghel1932deutsche,givon1985iconicity,bybee-morphology-1985,rijkhoff-explaining-1990,hawkins1994performance,hawkins2004efficiency,hawkins2014crosslinguistic,temperley-minimizing-2018}.
The idea is that language should be structured to reduce long-term dependencies of various kinds, by placing elements that depend on each other close to each other in linear order.
That is, elements of utterances which are more `relevant' or `mentally connected' to each other are closer to each other.
%For example, \citet{hawkins-efficiency-2003,hawkins2014crosslinguistic} has used a locality principle, `domain minimization,' to explain cross-linguistic universals of word order \citep[see also][for reviews of this idea and its predictions]{liu-dependency-2017,temperley-minimizing-2018}.

Our contribution is to present a new, highly general formalization of the relationship between sequential order and incremental memory in language processing, from which we can derive a precise and empirically testable version of the idea that utterance elements which depend on each other should be close to each other. 
%We do so by introducing a new information-theoretic formalization of the relationship between sequential order and memory usage in language processing. 
Our formalization allows us to predict the order of words within sentences, and morphemes within words directly by the minimization of memory usage.

We formalize the notion of memory constraints in terms of what we call the \key{memory--surprisal tradeoff}: the idea that the ease of comprehension depends on the amount of computational resources invested into remembering previous linguistic elements, e.g., words. 
Therefore, there exists a tradeoff between the quantity of memory resources invested, and the ease of language processing.
The shape of this tradeoff depends on the grammar of a language, and in particular the way that it structures information in time.
We characterize memory resources using the theory of lossy data compression \citep{cover2006elements,berger2003rate}.%, which has recently had success in predicting cognitive phenomena \citep{sims2018efficient,zenon2019information}.%, and we show how the tradeoff described by our general theory is manifested in different ways in existing mechanistic theories of language processing. 

Within our framework, we prove a theorem showing that lower memory requirements result when utterance elements that depend on each other statistically are placed close to each other. 
This theorem does not require any assumptions about the architecture or functioning of memory, except that it has a bounded capacity.
Using this concept, we introduce the \key{Efficient Tradeoff Hypothesis}: Order in natural language is structured so as to provide efficient memory--surprisal tradeoff curves.
We provide evidence for this hypothesis in three studies.
We demonstrate that word orders with short dependencies do indeed engender lower working memory resource requirements in toy languages studied in the previous literature, and we show that real word orders in corpora of 54 languages have lower memory requirements than would be expected under artificial baseline comparison grammars. 
Finally, we show that we can predict the order of morphemes within words in two languages using our principle of the minimization of memory usage.

Our work not only formalizes and tests an old idea in functional linguistics and psycholinguistics, it also opens up connections between those fields and the statistical analysis of natural language \citep{debowski-excess-2011,bentz2017entropy,lin-critical-2017}, and more broadly, between linguistics and fields that have studied information-processing costs and resource requirements in brains \citep[e.g.,][]{friston2010free} and general physical systems \citep[e.g.,][]{still2012thermodynamic}. 





