
Natural language solves the problem of allowing humans to communicate effectively given the limited cognitive resources available for producing and understanding utterances. Here, we investigate whether human languages are grammatically structured in a way that reduces the cognitive resource requirements for production and comprehension. In particular, we study whether the order of elements in linguistic sequences are structured so as to reduce incremental memory requirements. We do so by introducing a new information-theoretic formalization of the relationship between sequential order and memory usage in language processing. Our formalization allows us to predict the order of words within sentences, and morphemes within words, by the minimization of memory usage. We argue that our result explains an old intuition in linguistics: that elements of utterances which are more `relevant' or `mentally connected' to each other are closer to each other.

The suggestion that the structure of human language reflects a need for efficient processing under resource limitations has been present in the linguistics and cognitive science literature for decades~\citep{yngve1960model,berwick1984grammatical,hawkins1994performance,jaeger2011language,chomsky2005three,gibson2019efficiency,hahn2020universals}. The idea has been summed up in \citeauthor{hawkins1994efficiency}'s (\citeyear{hawkins1994efficiency}) \key{Performance--Grammar Correspondence Hypothesis} (PGCH), which holds that grammars are structured so that the typical utterance is easy to produce and comprehend under performance constraints.

One major source of resource limitation in language processing is incremental memory use. 
When producing and comprehending language in real time, a language user must keep track of what she has already produced or heard in some kind of incremental memory store, which is subject to resource constraints.
These memory constraints have been argued to underlie various \key{locality principles} which linguists have used to predict the orders of words within sentences and morphemes within words.
The idea is that language should be structured to reduce long-term dependencies of various kinds, by placing elements that depend on each other close to each other in linear order.
For example, \citet{hawkins-efficiency-2003,hawkins-crosslinguistic-2014} has used a locality principle called `domain minimization' to explain cross-linguistic universals of word order \citep[see also][for reviews of this idea and its predictions]{liu-dependency-2017,temperley-minimizing-2018}.

Our contribution is to present a new, highly general formalization of the relationship between sequential order and incremental memory in language processing, from which we can derive a precise and empirically testable version of the idea that utterance elements which depend on each other should be close to each other. 

We formalize the notion of memory constraints in terms of what we call the \key{memory--surprisal trade-off}: the idea that the ease of comprehension and the accuracy of production depend on the amount of computational resources invested into remembering previous words.
Therefore, there exists a trade-off between the quantity of memory resources invested, and the ease of language processing.
The shape of this trade-off depends on the grammar of a language, and in particular the way that it structures information in time.
We characterize memory resources using the theory of lossy data compression \citep{cover2006elements,berger}, which has recently had success in predicting cognitive phenomena \citep{sims2018efficient,gershman}, and we show how the trade-off described by our general theory is manifested in different ways in existing mechanistic theories of language processing. 

Within our framework, we prove a theorem showing that lower memory requirements result when utterance elements that depend on each other statistically are placed close to each other. This theorem does not require any assumptions about the architecture or functioning of memory, except that it has a bounded capacity. We demonstrate that word orders with short dependencies do indeed engender lower working memory resource requirements in toy languages from previous literature, and we show that real word orders in corpora of 54 languages have lower memory requirements than would be expected under artificial baseline comparison grammars. Finally, we show that we can predict the order of morphemes within words in two languages using our principle of the minimization of memory usage.

Our work not only formalizes and tests an old idea in functional linguistics and psycholinguistics, it also opens up connections between those fields and the statistical analysis of natural language \citep{debowski-excess-2011,bentz2017word,lin-critical-2017}, and more broadly, between linguistics and fields that have studied information-processing costs and resource requirements in brains \citep{friston}, organisms \citep{england}, and general physical systems \citep{still2012thermodynamic}. 





