
Natural language solves the problem of allowing humans to communicate effectively given the limited cognitive resources available for producing and understanding utterances. Here, we investigate whether human languages are grammatically structured in a way that reduces the cognitive resource requirements for production and comprehension. In particular, we study whether the order of elements in linguistic sequences are structured so as to reduce incremental memory requirements. We do so by introducing a new information-theoretic formalization of the relationship between sequential order and memory usage in language processing. Our formalization allows us to demonstrate that the memory resource requirements for real human language are less than would be expected from baselines, at the levels of word order, morpheme order, and the orders of phonemes and syllables within words. Our result formalizes an old intuition in linguistics: that elements of utterances which are more `relevant' or `mentally connected' to each other are closer to each other.

The suggestion that the structure of human language reflects a need for efficient processing under resource limitations has been present in the theoretical and functional linguistics literature for decades~\citep{yngve1961,berwick1984grammatical,hawkins1994performance,jaeger2011language,chomsky2005three,gibson2019efficiency}. The idea has been summed up in \citeauthor{hawkins1994efficiency}'s (\citeyear{hawkins1994efficiency}) \key{Performance--Grammar Correspondence Hypothesis} (PGCH), which holds that grammars are structured so that the typical utterance is easy to produce and comprehend under performance constraints.
%The idea has also been controversial, with prominent claims that efficiency in this sense has had no or only a minimal effect on the structure of human language \citep{}.

One major source of resource limitation in language processing is incremental memory use. 
When producing and comprehending language in real time, a language user must keep track of what she has already produced or heard in some kind of incremental memory store, which is subject to resource constraints.
Typically, these memory constraints manifest as processing difficulty associated with long-term dependencies.
For example, at the level of word-by-word online language comprehension, there is observable processing difficulty at moments when it seems that information about a word must be retrieved from working memory. 
This difficulty increases when there is a great deal of time or intervening material between the point when a word is first encountered and the point when it must be retrieved from memory  \citep{gibson1998syntactic,gibson1999memory,gibson2000dependency,mcelree,lewis2005activationbased,bartek2011search,nicenboim2015working}. 
This is to say that language comprehension is harder from humans when words which depend on each other for their meaning are separated by many intervening words.

Because of the documented difficulty associated with long-term dependencies among words, it has been hypothesized that working memory limitations create a pressure for \key{locality} in word order and in other domains. Locality means that linguistic elements that are dependent on each other will typically appear close to each other in time. In the domain of word order, this idea is called \key{dependency locality} and there is ample evidence from corpus statistics indicating that dependency locality is a real property of word order across many languages \citep[for recent reviews of this idea and the evidence for it, see][]{liu-dependency-2017,temperley-minimizing-2018}. Dependency locality corresponds to \citet{hawkins2004efficiency}'s principle of Domain Minimization, and it has the capacity to explain far-reaching universal properties of word order across languages. 

The idea of a locality pressure is not limited to word order, however. % TODO phonotactics lit review on SL languages?

Our contribution is to present a new, highly general formalization of the relationship between sequential order and incremental memory in language processing. We formalize the notion of memory constraints in terms of what we call the \key{memory--surprisal tradeoff}: the idea that it is possible to achieve greater ease of comprehension, and greater accuracy in production, at the cost of investing more computational resources into remembering previous words.
The shape of this tradeoff depends on the grammar of a language, and in particular the way that it structures information in time.
We characterize memory resources in a theory-neutral, information-theoretic manner based on the theory of lossy data compression \citep{cover2006elements,berger}, and show how the trade-off described by our general theory is manifested in different ways in existing mechanistic theories of language processing. % cite the crutchfield nature review, and predictive info bottleneck

% TODO: Modify paragraph below depending on what results we end up including
Within our framework, we prove a theorem showing that shorter dependencies coincide with lower memory requirements, and we demonstrate that word orders with short dependencies do indeed engender lower working memory resource requirements in toy languages from previous literature. Finally, we show that real word orders in corpora of TODO languages have lower memory requirements than would be expected under artificial baseline comparison grammars. 

Our work not only formalizes and tests an old idea in functional linguistics and psycholinguistics, it also opens up connections between those fields and the statistical analysis of natural language \citep{debowski-excess-2011,bentz2017word,lin-critical-2017}, and more broadly, between linguistics and fields that have studied information-processing costs and resource requirements in brains \citep{friston}, organisms \citep{england}, and physical systems from a thermodynamic perspective \citep{still2012thermodynamic}. %For example, our memory--surprisal tradeoff is a variant of the Predictive Rate--Distortion curve introduced by \citet{still-optimal-2010} and studied by \citet{marzen-nearly-2017}. 








%Complementing this work, a number of researchers in linguistic typology and corpus linguistics have shown how various universal properties of grammars can be explained in terms of memory-based processing difficulty \citep{hawkins2004efficiency,hawkins2014crosslinguistic,ferrericancho2006syntactic,gildea2010grammars,futrell2015largescale,liu2017dependency,temperley2018dependency}.
%We will see that one of the most successful theories in this domain, the principle of \key{dependency length minimization}, falls out of our formalization of processing efficiency as a special case.
%Our work formalizes and generalizes this previous work.




%For each amount of memory invested, a language permits a certain level of ease of processing 
%The resulting tradeoff of memory and surprisal of depends on the word order properties of a language. Analogous results also hold for language production by resource-constrained speakers. We show evidence that the preferred word orders of natural languages are those that enable efficient memory--surprisal tradeoffs.



%These strategies can be seen as sets of rules that differ in the way ...
%Languages differ considerably in the rules they apply to order information: English orders the object after the verb, Japanese places it before the verb \note{make more explicit here}.
%Explaining the variation in the strategies languages use to  order underlying hierarchical structures into linear strings of words has been one of the foci of linguistic research (\jd{CITE}).
%We show that these strategies reflect an underlying optimization principle: human languages are adapted to limitations in human working memory.

%We test the hypothesis that human languages represent different solutions to the problem of efficient computation with constrained working memory.






%\mhahn{original intro}
%Since the 1950s, it has been a persistent suggestion that human language processing is shaped by a resource bottleneck in short-term memory.
%Language is produced and comprehended incrementally in a way that crucially requires both speaker and listener to use an active memory store to keep track of what was previously said.
%Since short-term memory of this kind is known to be highly limited in capacity \citep{miller1956magical}, it makes sense for these capacity limits to comprise a major constraint on production and comprehension.
%Indeed, a great deal of work in sentence processing has focused on characterizing the effects of memory constraints on language processing \citep{gibson-linguistic-1998,lewis-activation-based-2005,levy2013memory}.

%At the same time, the field of functional linguistics has argued that these resource constraints not only affect online language processing, they also shape the form of human language itself.
%For example, the Performace--Grammar Correspondence Hypothesis (PGCH) of \citet{hawkins1994performance} holds that forms which are practically easier to produce and comprehend end up becoming part of the grammars of languages, and that this process can explain several of the universal properties of human languages originally documented by \citet{greenberg-universals-1963}.




