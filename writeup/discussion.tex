\paragraph{The Roles of Speakers, Listeners, and Interaction}
Our derivation of the memory-surprisal tradeoff considers the listener.

Debate about the roles of speaker and listener in shaping language.

Interaction: We could make the process include the entire conversational history (including what the listener said before).


\paragraph{`hockeystick problem'} conceptual issue: we're agnostic as to whether the actual full surprisal is different between real and baseline (we're restricting context length to $T$, and networks might not extract all the available information). all we're saying is that, in the setting of small memory budgets, real languages provide lower surprisals.

\paragraph{Nature of the Bound}
We have a lower bound, not an exact estimate of the tradeoff curve

\paragraph{Extralinguistic Context}
The assumption about information flow disregards the role of information sources that are external to the linguistic material in the sentence.
For instance, the interlocutors might have common knowledge of the weather, and the listener might use this to construct predictions for the speaker's utterances, even if no relevant information has been mentioned in the prior discourse.
Such sources of information are disregarded in our model.

\paragraph{Capacity vs Retrieval}
Our theoretical analysis places the main memory bottleneck in the capacity of short-term memory.
Not all models of memory in sentence processing make this assumption.
Indeed, there is evidence that difficulty of retrieving items is an important bottleneck in sentence processing.
In SI Section X, we show that our theoretical bounds are also compatible with a retrieval-based model such as ACT-R.


\paragraph{Limitations of Grammar Model}

\paragraph{absolute numbers, how they relate to what's known about human memory}

\subsection{Relation to Models of Sentence Processing}

There is a substantial literature proposing sentence processing models and quantitative memory metrics for sentence processing.
In this section, we discuss how our theoretical results relate to and generalize these previously proposed metrics.
We do not view our model as competing with or replacing any of these models; instead, our information-theoretic analysis captures aspects that are common to most of these models and shows how they arise from very general modeling assumptions.

Many proposals for memory metrics are based on the \emph{number} of objects simultaneously held in a stack-like data structure of memory, and \emph{how long} they are stored.

Our theoretical result can be seen as an information-theoretic version of such metrics.
It differs from these in two ways:
First, it considers not only the cost of maintaining direct syntactic dependencies, but extends this to any statistical relations between words that can be exploited for prediction.
Second, it weights these relations by the amount of predictive information shared between the words.

head-dependent mutual information hypothesis
In an idealized model \emph{all} predictive information is mediated through direct syntactic dependencies, and each dependency contributed an equal amount of predictive information, our memory bound would indeed become equivalent to such a metric.
Note that the average number of objects held in memory simultaneously is equivalent to the average time they are stored.



\cite{yngve1960model} production model with memory complexity measure (but problematically predicts left-branching structures to be hard, Kimball (1973))

\cite{miller-finitary-1963}: degree of self-embedding limited

\cite{frazier1985syntactic} local nonterminal count

Kimball (1973): Principle of Two Sentences

Rambow and Joshi 1994 using TAG

Marcus 1980 deterministic parsing

(Sabrina Gerth, Memory Limitations in Sentence Processing)

\cite{gerth2009unifying}


\cite{wanner1978atn}
\cite{frazier1978sausage}

\cite{rambow201512}: count words on stack

\cite{boston2012computational}

\cite{just1992capacity}

\cite{marcus1978theory}: deterministic parser with small lookahead

\paragraph{Dependency Locality Theory}
The quantity described in Proposition~\ref{prop:lower-bound} is formally similar to Storage Cost in the Dependency Locality Theory (DLT) \citep{gibson-linguistic-1998}: Storage cost at a given timestep is defined as the number of predictions that are held in memory.
Storage cost only considers predictions that are certain, and each prediction takes an equal amount of memory.
In contrast, the result in Proposition~\ref{prop:lower-bound} can be seen as weighting predictions by their certainty and the amount of predictive information.
In this sense, DLT storage cost can be seen as an approximation to Proposition~\ref{prop:lower-bound}.
DLT integration cost can be seen as surprisal given an imperfect memory representation, following \cite{futrell-noisy-context-2017}.

\paragraph{Cue-Based Retrieval Models}
- small buffer, unbounded unstructured store of chunks

- content-based retrieval

- interference


In cue-based retrieval models, memory effects arise from retrieval interference, whereas surprisal effects arise from backtracking (CITE).
Our model implements a bound on capacity, which is usually understood to be different from the presence of retrieval interference.
See SI Section X for a version of our model that more closely parallels cue-based retrieval models.

could also have more extensive discussion of the links here instead of the SI


\paragraph{Lossy-Context Surprisal}
\citet{futrell-noisy-context-2017} describe a processing model where listeners make predictions (and incur surprisal) based on lossy memory representations.
In particular, they consider loss models that delete, erase, or replace words in the past.
Under the assumption that loss affects words more strongly that are further in the past, they derive a principle of information locality:
A listener will incur surprisal
$$ -\log P(w_t) - \sum_{j=1}^{t-1} f(i-j) pmi(w_i; w_j) + R$$
where the `survival probability' $f(d)$ decreases as the distance $d$ between two words increases, and $R$ is a remainder term that can be argued to be small.
Given that $f$ is assumed to be decreasing, this prediction loss will be smaller when words with high mutual information are closer together in the input.
Our Proposition~\ref{prop:suboptimal} can be seen as an analogous result for general models of memory.


\paragraph{Metrics based on Minimalist Parser Models}
memory metrics based on minimalist parsers

\cite{kobele2013memory}
\cite{graf2014evaluating}
\cite{GrafEtAl15MOL}
\cite{gerth2015memory}
\cite{GrafEtAl17JLM}
\cite{desanto2020parsing}
based on counting how long a node is kept in memory and how many nodes are kept in memory

Linear or ranked combinations of versions of these metrics have been argued to account for various complexity and acceptability differences in complex embeddings.

\paragraph{Models unifying memory and surprisal}


evaluating memory metrics and surprisal on reading measures \cite{boston2008parsing} \cite{demberg2008data} \cite{boston2011parallel}

- Demberg et al: unified parsing model with both integration and surprisal costs \cite{demberg2009computational,demberg2013incremental}

- lossy-context surprisal: the perspective taken here


\subsection{Statistical Studies of Language}

\paragraph{Statistical Complexity}
Our formalization of listener memory is related to studies of dynamic systems in the Physics literature.
The tradeoff between listener memory and surprisal is formally equivalent to the \emph{Recursive Information Bottleneck} considered by \cite{still-information-2014}.
In the limit of optimal prediction and minimal surprisal, our formalization of listener memory is equivalent to the notion of \emph{Statistical Complexity} \citep{crutchfield-inferring-1989}.
In the limit $T \rightarrow \infty$, the quantity in (\ref{eq:memory}) is equal to the \emph{excess entropy} \citep{crutchfield-inferring-1989}.
However, the link between memory and information locality provided by our Theorem~\ref{prop:suboptimal} appears to be a novel theoretical contribution.
Relatedly, \cite{sharan-prediction-2016} shows a link between excess entropy and approximability by $n$-th order Markov models, noting that processes with low excess entropy can be approximated well with Markov models of low order.


\paragraph{Decay of Mutual Information}
In Propositions~\ref{prop:lower-bound} and \ref{prop:suboptimal}, we showed a close link between memory and the decay of \emph{conditional} mutual information $I_t := I[w_t, w_0 | w_{1\dots t-1}]$.
Prior work has studied the decay of \emph{unconditional} mutual information $I[w_t, w_0]$ in natural language \citep{ebeling-entropy-1994,lin-critical-2017}, and linked it to locality and memory \citep{futrell-noisy-context-2017}.

The decay of unconditional mutual information is less closely linked to memory requirements than conditional mutual information:
While the decay of conditional mutual informations provides a lower bound on memory need, unconditional mutual information does not:
Consider the constant process where with probability 1/2 all $w_t = 0$, and with probability 1/2 all $w_t = 1$. %%$w_t = c$, where $c$ is random but independent of $t$ for each specific draw from the process.
The unconditional mutual information is 1 at all distances, so does not decay at all, but the process only requires 1 bit of memory.
Conversely, one can construct processes where the unconditional mutual informations are 0 for all $t$, but where $P > 0$ and this predictive information is actually spread out over arbitrarily large distances (that is, the ratio of memory $M$ and predictability $P$ can be made arbitrarrily large).\footnote{First, consider the process (called X by REF) consisting of 2 random bits and their XOR. This one has bounded nonzero $J$, but zero unconditional MI. To get unbounded $J$, consider the following process for any $N \in \mathbb{N}_{>2}$: Every $w_t$ is equal to the XOR of $w_{t-1}$ and $w_{t-N}$, such that each $w_t$ has $Bernoulli(1/2)$ as its marginal. The unconditional mutual information between any two timesteps is zero, but modeling the process requires $N$ bits of memory.}



\paragraph{Long-range dependencies in text}    % excess entropy
\cite{debowski-excess-2011} has studied the excess entropy of language across long ranges of text, in particular studying whether it is finite. % compute excess entropy in text
Our work contrasts with this work in that we are interested in dependencies within sentences.


\paragraph{Decay vs Interference}
Work has suggested that interference and memory overload is more appropriate than decay \cite[p. 408]{lewis-activation-based-2005} for modeling locality and memory in sentence processing.
The bounds in Propositions~\ref{prop:lower-bound} and \ref{prop:suboptimal} hold for any type of memory model, and are thus compatible with decay- or interference-based models.
The formula in (\ref{eq:memory-bound}) might suggest that boundedness of memory entails that memory has to decay.
This is not the case:
A long dependency can be maintained perfectly with low average memory:
Informally, if every sentence is $N$ words long and has one long-distance dependency spanning the entire sentence, this dependency can be modeled perfectly with a memory cost that is independent of $N$.
In contrast, if every symbol strongly and non-redundantly depends on the character $T$ steps in the past, with $T$ large, this will create a memory cost proportional to $T$.




\paragraph{Memory and Hierarchical Structure; Finiteness of Memory}
Processing nontrivial hierarchical structures typically requires unbounded amounts of memory.
However, crucially, the \emph{average} memory demand for prediction can be finite, if the probability mass assigned to long dependencies is small.
For instance, languages defined by Probabilistic Context Free Grammars (PCFG) always have finite average memory.
The reason is that PCFGs assign low probabilities to long sequences.\footnote{Proposition 2 in \cite{chi-statistical-1999} implies that words drawn from a PCFG have finite expected length. This implies that average memory demands are finite.}



%\paragraph{Center Embeddings}
%\cite{miller-finitary-1963} attributed the unacceptability of multiple center-embedding to memory limitations.
%\cite{gibson-linguistic-1998}
%\paragraph{Other Psycholinguistic Predictions}
% RF: the fact that you would get locality effects given medium WM capacity, but not very high or very low WM capacity, as Bruno Nicenboim found. And maybe some speaker-listener asymmetries. 
%\paragraph{Speakers}
% RF: what matters for the speaker is not I[w_t, w_0 | w_1, …, w_{t-1}], but I[w_t, w_0 | w_1, …, w_{t-1}, G] where G is some representation of the speaker’s goal (like in the van Dijk paper). This changes the interpretation of the mutual information. For the listener, it’s just redundancy. For the speaker, it’s redundancy *conditional on the goal*—which you could interpret as something like conceptual relatedness of linguistic elements. Then the speaker’s pressure is to keep conceptually related things close. 





