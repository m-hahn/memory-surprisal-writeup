We introduced a notion of memory efficiency in language processing, called the memory--surprisal trade-off, and we showed that the orders of words and morphemes across languages are predicted by a process of optimizing this trade-off. \jd{bring it back to the Main Hypothesis}

Here, we discuss the limitations  of our results and the implications they have more broadly for the fields of psycholinguistics, typology, and information theory.


%\paragraph{The Roles of Speakers, Listeners, and Interaction}
%Our derivation of the memory-surprisal tradeoff considers the listener.
%Debate about the roles of speaker and listener in shaping language.
%Interaction: We could make the process include the entire conversational history (including what the listener said before).

\subsection{Implications}\jd{maybe change this header to sth more informative}
Our results show that the order of words and morphemes provides efficient memory--surprisal tradeoffs.
We also showed that this predicts the known preference for short dependencies.
Furthermore, we showed that, in two languages (Japanese and Sesotho), the order of verb affixes can partly be predicted by optimizing for the efficiency of the memory--surprisal tradeoff. \jd{this first paragraph is more of a summary of the results rather than implications of those results -- i'd move it up before this sub-header}


Our results leave open the causal mechanism leading to the observed optimization.
Does optimization for processing efficiency reflect biases in language acquisition \citep[e.g.][]{fedzechkina2012language, culbertson2012learning, fedzechkina-human-2017}, or in language production \citep[e.g.][]{macdonald2013language}? \jd{isn't there a third option, namely language processing/comprehension? i thought i remembered the fedzechkina work being agnostic about these options as well, no?}
It is possible that memory efficiency makes languages more learnable, as learning should require less memory resources for languages with more efficient memory-surprisal tradeoffs.
To the extent that universal-grammar based explanations make reference to computational efficiency \citep{chomsky2005tree}, such learning biases may be compatible with theories deriving word order patterns from innate biases.
To the extent that optimization reflects biases that come into play in language production and comprehension, they may reflect production-internal pressures to minimize effort on the speaker's part during sentence planning \citep{macdonald2013language}\jd{is this one of the availability-based production papers? in which case you might also cite ferreira and dell 2000. note that i also modified the previous sentence to reflect the option more clearly that this could just be an epiphenomenon of speaker least effort processes}, or an effort on the side of the speaker to produce utterances that are easy to comprehend by listeners. \mhahn{cite audience design}\jd{could cite eg brennan and clark or some of herb's earlier audience design papers}
Our results are compatible with either mechanism. \jd{what about the third option, that language processing might be set up to not be able }


%Furthermore, to what extent do languages change in ways that increase or preserve memory efficiency over time, and to what extent do speakers actively structure their production in ways that increase memory efficiency?

\mhahn{TODO is there more to cite here?}

%While our work has shown that certain word-order universals can be explained by efficiency in communication, we have made a number of basic assumptions about how language works in constructing our word-order grammars: for example, that sentences can be syntactically analyzed into trees of syntactic relations. We believe a promising avenue for future work is to determine whether these more basic properties themselves might also be explainable in terms of efficient communication.


%There is both corpus evidence and experimental evidence that long dependencies tend to be easier to process in head-final contexts \cite{futrell2017memory}.
%This is not predicted by the Information Locality Theorem, because it inherently

\mhahn{at some point we can say the following. where?}
Our results are compatible with work suggesting that memory strategies adapt to language structure and language statistics, and that experience can reduce memory difficulty.
This idea is supported by experimental work showing that forgetting effects in nested head-final dependencies are reduced or absent in  head-final structures  \citep{macdonald2002reassessing, christiansen1999toward, christiansen2009usage, vasishth2010short, engelmann2009processing, frank2019judgements, frank2016cross}, which can be explained in terms of surprisal given noisy memory representations \citep{futrell2020lossy}.
In a corpus study, \citet{yadav2020word} found that languages tolerate longer dependencies when their direction aligns with the default word order in that language. This is predicted by our information-theoretic model if we hypothesize that dependencies aligning with the default order tend to be more predictable, reducing surprisal compared to dependencies going in the non-default direction. Increased memory load for longer dependencies may therefore be compensated for by reduced surprisal.\jd{not sure i understand this last bit}

%; Hsiao  MacDonald, 2013; Levy, 2008, 2013;
%(Husain et al., 2014; Konieczny, 2000; Levy & Keller, 2013; Miyamoto & Nakamura, 2003; Nakatani  Gibson, 2010; Vasishth  Lewis, 2006)


\subsection{Limitations}

\paragraph{Finiteness of Data}
As corpora are finite, estimates for $I_t$ may not be reliable for larger values of $t$.
In particular, we expect that models will underestimate $I_t$ for large $t$, as models will not be able to extract and utilize all available information over longer distances.
This has implications for the interpretation of the memory--surprisal tradeoffs at higher values of memory entropy $H_M$.
In Experiment 1, the lowest achieved surprisals are different for real and baseline orderings.
This need not indicate that, in the limit of very large memory entropies $H_M$, these orderings still have different surprisals $S_\infty$:
It is logically possible that real and baseline languages actually have the same surprisal $S_\infty$, but that baseline orderings spread the same amount of predictive information over a larger distance, making it harder for models to extract given finite corpus data.
%Another possibility is that that surprisal differs across all possible memory entropes $H_M$; in this case, $S_\infty$ would be different for real and baseline orderings.
%Our results are also compatible in principle with a situation where the difference in surprisal \emph{reverses} at high values of memory entropy $H_M$.
What our results do imply, though, is that real languages provide lower surprisals in the setting of relatively small memory budgets. \jd{can we say sth more concrete about which estimates are more vs less trustworthy?}


\paragraph{Nature of the Bound}
Our theoretical result provides a lower bound on the tradeoff curve that holds across all ways of physically realizing a memory representation obeying the postulates (1-3).
However, this bound may be loose in two ways:
First, architectural properties of human memory might introduce additional constraints on possible representations.
Depending on the role played by factors other than infomation-theoretic capacity, the tradeoffs achieved by these human memory representations need not be close to achieving the theoretical bounds.
Furthermore, depending on properties of the stochastic process, the bound might be loose across all models, that is, there are processes where the bound is not attainable by any memory architecture.
We provide an artificial example with analytical calculations in SI Section \REF; however, that example does not seem linguistically natural.

\paragraph{Extralinguistic Context}
Comprehension Postulate 1 states that the memory state after receiving a word is determined by that word and the memory state before receiving this word.
The assumption about information flow disregards the role of information sources that are external to the linguistic material in the sentence.
For instance, the interlocutors might have common knowledge of the weather, and the listener might use this to construct predictions for the speaker's utterances, even if no relevant information has been mentioned in the prior discourse.
Such sources of information are disregarded in our model.
They are also disregarded in many other models of memory in sentence processing.

%\paragraph{Capacity vs Retrieval}
%Our theoretical analysis places the main memory bottleneck in the capacity of short-term memory.
%Not all models of memory in sentence processing make this assumption.
%Indeed, there is evidence that difficulty of retrieving items is an important bottleneck in sentence processing.
%In SI Section X, we show that our theoretical bounds are also compatible with a retrieval-based model such as ACT-R.

\paragraph{Limitations of Grammar Model}
In Study 2, baseline grammars are constructed in a formalism that cannot fully express some word order regularities found in languages.
For instance, it cannot express orders that differ in main clauses and embedded clauses.
This is a limitation common to other order grammar formalisms considered in the literature \citep{gildea-optimizing-2007,futrell2015experiments,wang2016galactic}.
This limitation does not hold in Study 3, where the formalism provides very close fit to observed morpheme orders.


%\paragraph{absolute numbers, how they relate to what's known about human memory}

\subsection{Relation to Models of Sentence Processing}
\label{sec:sentprod-models}

There is a substantial literature proposing sentence processing models and quantitative memory metrics for sentence processing.
In this section, we discuss how our theoretical results relate to and generalize these previously proposed models.
We do not view our model as competing with or replacing any of these models; instead, our information-theoretic analysis captures aspects that are common to most of these models and shows how they arise from very general modeling assumptions. 
%In this section, we argue that---although our theory is couched in the language of surprisal theory \citep{hale2001probabilistic,levy2008expectation,hale2016information}---the memory--surprisal curve reflects fundamental information-theoretic trade-offs that must apply in any theory. 

In Section~\ref{sec:tradeoff}, we proved a formal relationship between the entropy of memory $H_M$ and average surprisal $S_M$. 
We made no assumptions about the architecture of incremental memory, and so our result is general across all such architectures.
Memory representations do not have to be rational or optimal for our bound in Theorem~\ref{prop:suboptimal} to hold.
There is no physically realizable memory architecture that can violate this bound.

However, psycholinguistic theories may differ on whether the entropy of memory $H_M$ really is the right measure of memory load, and on whether average surprisal $S_M$ really is the right predictor of processing difficulty for humans. Therefore, in order to establish that our information-theoretic processing model generalizes previous theories, we will establish two links:
\begin{itemize}
    \item Our measure of memory usage generalizes theories that are based on counting numbers of objects stored in incremental memory \citep[e.g.,][]{yngve1960model,miller-finitary-1963,frazier1985syntactic,gibson-linguistic-1998,kobele2013memory,graf2014evaluating,GrafEtAl15MOL,gerth2015memory,GrafEtAl17JLM,desanto2020parsing}. Furthermore, for theories where memory is constrained in its capacity for \emph{retrieval} rather than storage \citep[e.g.,][]{mcelree-memory-2003,lewis-activation-based-2005}, the information locality bound will still hold.
    \item Our predictor of processing difficulty (i.e., average surprisal) reflects at least a \emph{component} of the predicted processing difficulty under other theories.
\end{itemize}

Below, we discuss the connections between our theory and existing theories of human sentence processing with regard to the points above.

\paragraph{Storage-based theories}

There is a long tradition of models of human language processing in which difficulty is attributed to high working memory load. 
These models go back to \citet{yngve1960model}'s production model, where difficulty was associated with moments when a large number of items have to be kept on a parser stack; this model correctly predicted the difficulty of center-embedded clauses, but problematically predicted that left-branching structures should be hard \citep{kimball1973seven}. Other early examples include \citet{miller-finitary-1963} and \citet{frazier1985syntactic}'s measure of syntactic complexity based on counting the number of local nonterminal nodes. More recently, a line of literature has formulated complexity metrics based on how long and how many nodes are kept in incremental memory during parsing, and used linear or ranked combinations of these metrics to predict acceptability differences in complex embeddings \citep{kobele2013memory,graf2014evaluating,rambow201512,GrafEtAl15MOL,gerth2015memory,GrafEtAl17JLM,desanto2020parsing}.

%Our theoretical result can be seen as an information-theoretic version of such metrics.
%It differs from these in two ways:
%First, it considers not only the cost of maintaining direct syntactic dependencies, but extends this to any statistical relations between words that can be exploited for prediction.
%Second, it weights these relations by the amount of predictive information shared between the words.

Our measure of memory complexity---i.e., the memory entropy $H_M$---straightforwardly generalizes measures based on counting items stored in memory. If each item stored in memory requires $k$ bits of storage, then storing $n$ items would require a capacity of $nk$ bits in terms of memory entropy $H_M$. In general, if memory entropy is $H_M$ and all items stored in memory take $k$ bits each to store, then we can store $H_M/k$ items. 

The memory entropy $H_M$ generalizes this logic. The memory entropy $H_M$ can be interpreted as the number of bits of memory capacity required to store $n$ different objects, while allowing that different items stored in memory might take different numbers of bits to store, and also that the memory representation might be able to compress the representations of multiple items when they are stored together, so that the capacity required to store two items might be less than the sum of the capacity required to store each individual item. 

\paragraph{The Dependency Locality Theory}
The connection with the Dependency Locality Theory is particularly interesting.
Our lower bound on memory usage, described in Theorem~\ref{prop:suboptimal} Eq.~\ref{eq:memory-bound}, is formally similar to Storage Cost in the Dependency Locality Theory (DLT) \citep{gibson-linguistic-1998,gibson2000dependency}.
In that theory, storage cost at a given timestep is defined as the \emph{number of predictions} that are held in memory.
Our bound on memory usage is stated in terms of mutual information, which indicates the \emph{amount of predictive information} extracted from the previous context and stored in memory.
Therefore, our measure generalizes DLT storage cost.

%Our measure and goes beyond DLT storage cost in that the DLT quantity only considers predictions that are certain, and each prediction is postulated to take an equal amount of memory.
%In contrast, our measure of memory usage can be seen as weighting predictions by their certainty and amount of predictive information. 
%In this sense, DLT storage cost can be seen as an approximation to Eq.~\ref{eq:memory-bound}.

The other component of the DLT is integration cost, the amount of difficulty incurred by establishing a long-term syntactic dependency. 
In our framework, DLT integration cost corresponds to surprisal given an imperfect memory representation, following \cite{futrell2020lossy}.

There is one remaining missing link between our theory of processing difficulty and theories such as the Dependency Locality Theory:
our information locality theorem says that \emph{statistical} dependencies should be short-term, whereas psycholinguistic theories of locality have typically focused on the time-span of \key{syntactic dependencies}: words which depend on each other to determine the meaning or the well-formedness of a sentence. Statistical dependencies, in contrast, mean that whenever one element of a sequence determines or predicts another element \emph{in any way}, those two elements should be close to each other in time. 

If statistical dependencies, as measured using mutual information, can be identified with syntactic dependencies, then that would mean that information locality is straightforwardly a generalization of dependency locality. \citet{futrell2019syntactic} give theoretical and empirical arguments that this is so. They show that syntactic dependencies as annotated in dependency treebanks identify word pairs with especially high mutual information, and give a derivation showing that this is to be expected according to a formalization of the postulates of dependency grammar. The connection between mutual information and syntactic dependency has also been explored by \citet{de1996selection, yuret1998discovery}. % maximizing mutual information principle; de paiva alves; yuret


\paragraph{Cue-Based Retrieval Models}

In some psycholinguistic theories, memory-related difficulty arises not because of a bound on memory capacity, but rather because of difficulties involved in retrieving information from memory \citep{mcelree2000sentence,lewis-activation-based-2005,nicenboim2018models,vasishth2019computational}. We are able to prove an analogous theorem that applies to such theories: see Supplementary Material Section \REF for an extension of our analysis to the case involving a short-term memory (STM) with unbounded capacity, a working memory (WM) with limited capacity, and cost associated with communication between WM and STM. Essentially, the constraint on the the memory state in our theorem above can be re-interpreted as a constraint on the capacity of a communication channel linking STM to WM. In particular, this result constrains average surprisal for memory models based on cue-based retrieval such as the ACT-R model of \citet{lewis-activation-based-2005}.

The ACT-R model of \cite{lewis-activation-based-2005} does not have an explicit surprisal cost.
Instead, surprisal effects are interpreted as arising because, in less constraining contexts, the parser is more likely to make decisions that then turn out to be incorrect, leading to additional correcting steps.
%We see this as an algorithmic-level implementation of the justification for surprisal theory provided by \citet{levy2008expectation}:
We view this as an algorithmic-level implementation of a surprisal cost:
If a word $w_t$ is unexpected given the current state of the working memory---i.e., buffers and control states---then their current state must provide insufficient information to constrain the actual syntactic state of the sentence, meaning that the parsing steps made to integrate $w_t$ are likely to include more backtracking and correction steps.
Thus, we argue that cue-based retrieval models predict that the surprisal $- \log P(w_t|m_t)$ will be part of the cost of processing word $w_t$.

Work within cue-based retrieval frameworks has suggested that working memory is not characterized by a decay in information over time, but rather an accumulation of interference among similar items stored in memory \citep[][p. 408]{lewis-activation-based-2005}.
In contrast, the formula for memory usage in Eq.~\ref{eq:memory-bound} might appear to suggest that boundedness of memory entails that representations have to decay over time.
However, this is not the case:
our theorem does not imply that a listener forgets words beyond some amount of time $T$ in the past. 
An optimal listener may well decide to remember information about words more distant than $T$, but in order to stay within the bounds of memory, she can only do so at the cost of forgetting some information about words closer than $T$.
The Information Locality Lower Bound still holds, in the sense that the long-term dependency will cause processing difficulty, even if the long-term dependency is not itself forgotten.
See SI Appendix Section \REF for a mathematical example illustrating this phenomenon.

%y and non-redundantly depends on the character $T$ steps in the past, with $T$ large, this will create a memory cost proportional to $T$.

% TODO: Should we mention retrieval interference effects? If there is any slowdown that is *directly* attributable to similarity-based interference, then I think we can't capture it. But a lossily-compressed memory will have interference, in the sense that more "semantically" similar items in memory are more likely to be confused for each other.

\paragraph{The role of surprisal}

There are more general reasons to believe that any realistic theory of sentence processing must include surprisal as at least a \emph{component} of the cost of processing a word, even if it is not explicitly stated as such. 
%More formally, we argue that processing difficulty for a word $w_t$ must at least be given by $k (-\log P(w_t |m_t)) + R$, with $k$ a scaling factor giving the relative contribution of surprisal to processing cost, and $R$ representing all other factors. The scaling factor $k$ may be small, but it cannot be zero, for both empirical and theoretical reasons. 
There are both empirical and theoretical grounds for this claim.
Empirically, surprisal makes a well-documented and robust contribution to processing difficulty in empirical studies of reading times and event-related potentials \citep{smith2013effect,frank2015erp}. 
Theoretically, surprisal may represent an irreducible thermodynamic cost incurred by any information processing system \citep{landauer1961irreversibility,still2012thermodynamic,zenon2019information}, and there are multiple converging theoretical arguments for why it should hold as a cost in human language processing in particular \cite[see][for a review]{levy2013memory}. 


\mhahn{say something about models aiming to integrate memory and surprisal and why ours mostly generalizes them? approaches: (1) separate costs in a unified parsing model \citep{demberg-incremental-2013}, (2) surprisal given lossy memory \citep{futrell2020lossy}, (3) surprisal arising from backtracking as explained above \citep{lewis-activation-based-2005}. also: \citet{rasmussen2018left}: memory effects arising from interference in a distributed model of memory (essentially a capacity/precision constraint on $M$), surprisal effects arise from the cost of renormalization (essentially stipulated?).}


\paragraph{Previous information locality results}

Previous work has attempted to derive the principle of information locality from incremental processing models. 
\citet{futrell2020lossy} describe a processing model where listeners make predictions (and incur surprisal) based on lossy memory representations.
In particular, they consider loss models that delete, erase, or replace words in the past.
Within this model, they were able to establish a similar information locality result, by showing that the theoretical processing difficulty increases when words with high \emph{pointwise mutual information} are separated by large distances \citep{futrell2019information,futrell2020lossy}. Pointwise mutual information is the extent to which a \emph{particular value} predicts another value in a joint probability distribution. For example, if we have words $w_1$ and $w_2$ in a sentence, their pointwise mutual information is:
\begin{equation*}
    \text{pmi}(w_1; w_2) \equiv \log \frac{P(w_2|w_1)}{P(w_2)}.
\end{equation*}
Mutual information, as we defined it in Eq.~\ref{eq:mi}, is the \emph{average} pointwise mutual information over an entire probability distribution.

Our information locality bound theorem differs from this previous result in three ways:
\begin{enumerate}
    \item \citet{futrell2020lossy} required an assumption that incremental memory is subject to decay over time. In contrast, we do not require any assumptions about incremental memory except that it has bounded capacity (or that retrieval operations have bounded capacity; see above).
    \item Our result is a precise bound, whereas the previous result was an approximation based on neglecting higher-order interactions among words.
    \item Our result is about the fall-off of the mutual information between words, \emph{conditional on the intervening words}. The previous result was about the fall-off of \emph{pointwise} mutual information between specific words, without conditioning on the intervening words. We argue that conditional mutual information  is more relevant for measuring memory usage than unconditional mutual information, see Section \REF.
\end{enumerate}

As noted above, this previous work defined information locality in terms of the \emph{unconditional} (pointwise) mutual information between linguistic elements (see also Section \REF for further work studying unconditional mutual information.). 
Here we argue that conditional mutual information is more relevant for measuring memory usage than unconditional mutual information. 
While the decay of conditional mutual information provably provides a lower bound on memory entropy, the decay of unconditional mutual information does not.
In the Supplementary Materials Section TODO, we provide an example of a stochastic process where unconditional mutual information does not decay with distance, but memory requirements remain low.


\mhahn{also mention connectionist models of memory in incremental processing? such as \cite{macdonald2002reassessing}}

%\mhahn{TODO: \cite{qian-cue-2012}}

%head-dependent mutual information hypothesis
%In an idealized model \emph{all} predictive information is mediated through direct syntactic dependencies, and each dependency contributed an equal amount of predictive information, our memory bound would indeed become equivalent to such a metric.
%Note that the average number of objects held in memory simultaneously is equivalent to the average time they are stored.


\paragraph{Rate-Distortion Theory in Cognition}\mhahn{TODO this doesn't fit with the heading `memory of sentence processing'}
As described in Section~\ref{sec:formal-tradeoff}, the Memory-Surprisal Tradeoff instantiates the framework of rate-distortion theory.
This approach has previously been applied to visual working memory \citep{brady2009compression, sims2012ideal, sims2016rate}.
These studies model visual working memory as solving the problem of storing a maximal amount of information given fixed capacity, and provide experimental evidence that visual working memory can store more items when they show stronger statistical patterns in cooccurrence \citep{brady2009compression}, and that memory performance is better when encoded items show lower variance \citep{sims2012ideal}, both in accordance with the predictions of optimal compression.


\subsection{Efficiency for the listener, or efficiency for the producer?}
\mhahn{condense to one paragraph. can place this into the Implications.}
Our theory is stated in terms of the efficiency of language processing for a comprehender of language. However, we can show that an analogous memory--surprisal trade-off exists in language production. Below, we discuss the memory--surprisal trade-off from a production perspective, and consider how it might be possible to empirically distinguish the comprehender's trade-off from the producer's trade-off.

In the case of production, the memory--surprisal trade-off arises from the minimization of error in production of linguistic sequences. That is, given a \key{competence language} (a target distribution on words given contexts), a speaker tries to produce a \key{performance language} which is as close as possible to the competence language. The performance language operates under memory constraints, so the performance language will diverge from the competence language due to production errors. When a speaker has more incremental memory about what she has already produced, then she is able to produce linguistic sequences with less error, thus reducing the divergence between the performance language and the competence language. The reduction of this competence--performance divergence is formally equivalent to the minimization of average surprisal from Section~\ref{sec:listener-tradeoff}.

%\mhahn{I'm trying to put the math into the SI, TODO clean up a bit more}

Formally, we assign a speaker a production policy $q(w_t|m_t)$ that produces the next word conditional on the speaker's memory state $m_t$.
We assume that speakers aim to minimize the KL divergence from the performance language $q(w_t|m_t)$ to the target competence language $p(w_t|w_{<t})$. We call this divergence the \key{competence--performance divergence} under the memory encoding function $M$ and the production policy $q$:
    \begin{align}
    \label{eq:comp-perf-div}
    d^q_M &\equiv D_{\text{KL}} [ p(w_t|w_{<t}) || q(w_t|m_t) ] \\
        &= \sum_{w_{\le t}} p(w_{\le t}) \log \frac{p(w_t | w_{<t})}{q(w_t|m_t)}.
    \end{align}

Under this assumption, the Information Locality Bound Theorem will apply in production as well as comprehension (see SI Section \REF for a proof):
The competence-performance divergence $d_M^q$ trades off with memory load $H[m_t]$, and this tradeoff will be more favorable when languages exhibit information locality.
This means that languages that exhibit information locality can be produced with greater accuracy given limited memory resources.
See SI Section \REF for a formal statement of production postulates, and the proof of an information locality theorem in production.

\mhahn{TODO we need to cite something here?}

%We derive the existence of this trade-off from the following postulates about language production. Let the competence language be represented by a stationary stochastic process, parameterized by a probability distribution $p(w_t | w_{<t})$ giving the conditional probability of any word $w_t$ given an unbounded number of previous words. Our postulates describe a speaker who tries to find a performance language $q(w_t|m_t)$ to match the the competence language using incremental memory representations $m_t$:

%\begin{enumerate}
%    \item Production Postulate 1 (Incremental memory). At time $t$, the speaker has an incremental \key{memory state} $m_t$ that contains (1) her stored information about previous words that she has produced, and (2) information about her production target. The memory state is given by a \key{memory encoding function} $M$ such that $m_t = M(w_{t-1}, m_{t-1})$.
    
%    \item Production Postulate 2 (Production policy). At time $t$, the speaker produces the next word $w_t$ conditional on her memory state by drawing from a probability distribution $q(w_t | m_t)$. We call $q$ the speaker's \key{production policy}.
    
%    \item Production Postulate 3 (Minimizing divergence). The production policy $q$ is selected to minimize the KL divergence from the performance language to the target competence language $p(w_t|w_{<t})$. We call this divergence the \key{competence--performance divergence} under the memory encoding function $M$ and the production policy $q$:
%    \begin{align}
%    \label{eq:comp-perf-div}
%    d^q_M &\equiv D_{\text{KL}} [ p(w_t|w_{<t}) || q(w_t|m_t) ] \\
%        &= \sum_{w_{\le t}} p(w_{\le t}) \log \frac{p(w_t | w_{<t})}{q(w_t|m_t)}.
%    \end{align}
%    The production policy is then the solution to the functional minimization problem:
%    \begin{equation}
%        \mathop{\text{minimize }}_{q(w_t|m_t)} d^q_M.
%    \end{equation}
%\end{enumerate}

%Completing the link with the memory--surprisal trade-off in comprehension, we note that when the production policy $q(w_t|m_t)$ is selected to minimize the competence--performance divergence $d^q_M$, then this divergence becomes equal to the memory distortion $d_M$ discussed above in the context of comprehension costs. Therefore, under these postulates, the Information Locality Bound Theorem will apply in production as well as comprehension. This means that languages that exhibit information locality can be produced with greater accuracy given limited memory resources.

In the case of language comprehension, the trade-off represented excess processing \emph{difficulty} arising due to memory constraints. In the case of language production, the trade-off represents \emph{production error} arising due to memory constraints. When memory is constrained, then the speaker's productions will diverge from her target language. And as memory is more and more constrained, this divergence will increase more and more. The degree of divergence is measured in the same units as surprisal, hence the formal equivalence between the listener's and speaker's memory--surprisal trade-offs. 

Although the memory--surprisal trade-off is mathematically similar between comprehension and production, it is not necessarily identical. The comprehender's memory--surprisal trade-off has to do with the amount of predictive information $I_t$ stored in memory, where $I_t$ is defined in terms of a probability distribution on words given $t$ words of context. In the producer's memory--surprisal tradeoff, this probability distribution may be different, because the producer has knowledge of a production target (Production Postulate 1). Nevertheless, if the producer's probability distribution is similar to the comprehender's, then we predict the same trade-off for the producer as for the comprehender.

It may be possible to use this asymmetry to distinguish whether word and morpheme order is more optimized for the comprehender or the producer. If word order is best predicted under a probability model that uses zero information about a production target (as in the current work), then we have evidence that the comprehender's trade-off is more important. On the other hand, if word order is best predicted under a probability model that uses (partial) information about a production target, then we have evidence that the producer's trade-off is more important. As estimating the difference between these probabilility distributions is difficult, we leave this avenue of research to future work.

\subsection{Information-theoretic studies of language}

Our work opens up a connection between psycholinguistics, linguistic typology, and statistical studies of language. Here, we survey the connections between our work and previous statistical studies.



Our formalization of memory is related to studies of dynamic systems in the physics literature.
Our memory--surprisal curve is closely related to the \emph{predictive information bottleneck} introduced by \citet{still-information-2014} and studied by \citet{marzen-predictive-2016}; in particular, it is a version of the \emph{recursive information bottleneck} \citep[][\S 4]{still-information-2014}. 
In the limit of optimal prediction and minimal surprisal, our formalization of memory cost is equivalent to the notion of \emph{Statistical Complexity} \citep{crutchfield-inferring-1989,shalizi2001computational}; this is the minimum value of $H_M$ that achieves $S_M = S_\infty$.
In the limit $T \rightarrow \infty$, the quantity in Eq.~\ref{eq:memory-bound} is equal to the \emph{excess entropy} \citep{crutchfield-inferring-1989}.
The link between memory and information locality provided by our Theorem~\ref{prop:suboptimal} appears to be a novel theoretical contribution.
Related to Theorem~\ref{prop:suboptimal}, \citet{sharan-prediction-2016} show a link between excess entropy and approximability by $n$-th order Markov models, noting that processes with low excess entropy can be approximated well with Markov models of low order.

\cite{debowski-excess-2011} studied the excess entropy of language across very long ranges of text, in particular aiming to determine whether it is finite.
Our work contrasts with this work in that we are interested in dependencies within sentences.
\cite{hahn2019estimating} empirically estimate the predictive rate-distortion tradeoff of natural language \citep{still-information-2014,marzen-predictive-2016}.
This tradeoff is related to but different from the memory-surprisal tradeoff: it models a setting where past observations are compressed once as a whole, while a memory capacity bottleneck holds at every point in time in our model.
Nonetheless, their method might also be applicable to estimating of memory-surprisal tradeoff curves.


While we define information locality in terms of \emph{conditional} mutual information, prior work has studied  how \emph{unconditional} mutual information decays with distance in natural language texts, at the level of orthographic characters \citep{ebeling-entropy-1994,lin-critical-2017} and words \citep{futrell2019syntactic}.
As described in Section \REF, \citet{futrell2020lossy} used unconditional mutual information to derive an information locality result.
Here we argue that conditional mutual information is more relevant for measuring memory usage than unconditional mutual information. 
While the decay of conditional mutual information provably provides a lower bound on memory entropy, the decay of unconditional mutual information does not.
In SI Section \REF, we provide an example of a stochastic process where unconditional mutual information does not decay with distance, but memory requirements remain low.


The entropy rate $S_\infty$ of natural language text has been studied by different authors, including \citet{shannon1951entropy,bentz2017entropy,takahashi2018cross}. 



The average surprisal of real and counterfactual word orders have been studied by \citet{gildea-human-2015} and \cite{hahn2020universals}.
\citet{gildea-human-2015} found that, in five languages, real orders provide lower trigram surprisal than baseline languages.
This work can be viewed as instantiating our model in the case where the encoding function $M$ records exactly the past two words, and showing that these five languages show optimization for surprisal under this encoding function.
\citet{hahn2020universals} compared surprisal and parseability for real and baseline orders as estimated using neural network models, arguing that word orders optimize a tradeoff between these quantities.
The results of Experiment 2 strengthen this by showing that real word orders optimize surprisal across possible memory capacities and memory encoding functions.




%\paragraph{Memory and Hierarchical Structure; Finiteness of Memory}
%Processing nontrivial hierarchical structures typically requires unbounded amounts of memory.
%However, crucially, the \emph{average} memory demand for prediction can be finite, if the probability mass assigned to long dependencies is small.
%For instance, languages defined by Probabilistic Context Free Grammars (PCFG) always have finite average memory.
%The reason is that PCFGs assign low probabilities to long sequences.\footnote{Proposition 2 in \cite{chi-statistical-1999} implies that words drawn from a PCFG have finite expected length. This implies that average memory demands are finite.}

% TODO: Does this only hold for proper PCFGs?



%\paragraph{Center Embeddings}
%\cite{miller-finitary-1963} attributed the unacceptability of multiple center-embedding to memory limitations.
%\cite{gibson-linguistic-1998}
%\paragraph{Other Psycholinguistic Predictions}
% RF: the fact that you would get locality effects given medium WM capacity, but not very high or very low WM capacity, as Bruno Nicenboim found. And maybe some speaker-listener asymmetries. 
%\paragraph{Speakers}
% RF: what matters for the speaker is not I[w_t, w_0 | w_1, …, w_{t-1}], but I[w_t, w_0 | w_1, …, w_{t-1}, G] where G is some representation of the speaker’s goal (like in the van Dijk paper). This changes the interpretation of the mutual information. For the listener, it’s just redundancy. For the speaker, it’s redundancy *conditional on the goal*—which you could interpret as something like conceptual relatedness of linguistic elements. Then the speaker’s pressure is to keep conceptually related things close. 




