\paragraph{The Roles of Speakers, Listeners, and Interaction}
Our derivation of the memory-surprisal tradeoff considers the listener.

Debate about the roles of speaker and listener in shaping language.

Interaction: We could make the process include the entire conversational history (including what the listener said before).


\paragraph{`hockeystick problem'} conceptual issue: we're agnostic as to whether the actual full surprisal is different between real and baseline (we're restricting context length to $T$, and networks might not extract all the available information). all we're saying is that, in the setting of small memory budgets, real languages provide lower surprisals.

logically different possibilities

- same asymptotic surprisal, different tradeoff

- same small-memory tradeoff, different asymptotic surprisal (difference might come out later)

- better small-memory tradeoff but worse high-memory tradeoff

\paragraph{Nature of the Bound}
We have a lower bound, not an exact estimate of the tradeoff curve

\paragraph{Extralinguistic Context}
The assumption about information flow disregards the role of information sources that are external to the linguistic material in the sentence.
For instance, the interlocutors might have common knowledge of the weather, and the listener might use this to construct predictions for the speaker's utterances, even if no relevant information has been mentioned in the prior discourse.
Such sources of information are disregarded in our model.

\paragraph{Capacity vs Retrieval}
Our theoretical analysis places the main memory bottleneck in the capacity of short-term memory.
Not all models of memory in sentence processing make this assumption.
Indeed, there is evidence that difficulty of retrieving items is an important bottleneck in sentence processing.
In SI Section X, we show that our theoretical bounds are also compatible with a retrieval-based model such as ACT-R.


\paragraph{Limitations of Grammar Model}

\paragraph{absolute numbers, how they relate to what's known about human memory}

\subsection{Relation to Models of Sentence Processing}
\label{sec:sentprod-models}

There is a substantial literature proposing sentence processing models and quantitative memory metrics for sentence processing.
In this section, we discuss how our theoretical results relate to and generalize these previously proposed models.
We do not view our model as competing with or replacing any of these models; instead, our information-theoretic analysis captures aspects that are common to most of these models and shows how they arise from very general modeling assumptions. 
%In this section, we argue that---although our theory is couched in the language of surprisal theory \citep{hale2001probabilistic,levy2008expectation,hale2016information}---the memory--surprisal curve reflects fundamental information-theoretic trade-offs that must apply in any theory. 

In Section~\ref{sec:tradeoff}, we proved a formal relationship between the entropy of memory $H_M$ and average surprisal $S_M$. 
We made no assumptions about the architecture of incremental memory, and so our result is general across all such architectures.
Memory representations do not have to be rational or optimal for our bound in Theorem~\ref{prop:suboptimal} to hold.
There is no physically realizable memory architecture that can violate this bound.

However, psycholinguistic theories may differ on whether the entropy of memory $H_M$ really is the right measure of memory load, and on whether average surprisal $S_M$ really is the right predictor of processing difficulty for humans. Therefore, in order to establish that our information-theoretic processing model generalizes previous theories, we will establish two links:
\begin{itemize}
    \item Our measure of memory usage generalizes theories that are based on counting numbers of objects stored in incremental memory \citep[e.g.,][]{yngve1960model,miller-finitary-1963,frazier1985syntactic,gibson-linguistic-1998,kobele2013memory,graf2014evaluating,GrafEtAl15MOL,gerth2015memory,GrafEtAl17JLM,desanto2020parsing}. Furthermore, for theories where memory is constrained in its capacity for \emph{retrieval} rather than storage \citep[e.g.,][]{mcelree-memory-2003,lewis-activation-based-2005}, the information locality bound will still hold.
    \item Our predictor of processing difficulty (i.e., average surprisal) reflects at least a \emph{component} of the predicted processing difficulty under other theories.
\end{itemize}

Below, we discuss the connections between our theory and existing theories of human sentence processing with regard to the points above.

\paragraph{Storage-based theories}

There is a long tradition of models of human language processing in which difficulty is attributed to high working memory load. 
These models go back to \citet{yngve1960model}'s production model, where difficulty was associated with moments when a large number of items have to be kept on a parser stack; this model correctly predicted the difficulty of center-embedded clauses, but problematically predicted that left-branching structures should be hard \citep{kimball1973}. Other early examples include \citet{miller-finitary-1963} and \citet{frazier1985syntactic}'s measure of syntactic complexity based on counting the number of local nonterminal nodes. More recently, a line of literature has formulated complexity metrics based on how long and how many nodes are kept in incremental memory during parsing, and used linear or ranked combinations of these metrics to predict acceptability differences in complex embeddings \citep{kobele2013memory,graf2014evaluating,rambow201512,GrafEtAl15MOL,gerth2015memory,GrafEtAl17JLM,desanto2020parsing}.

%Our theoretical result can be seen as an information-theoretic version of such metrics.
%It differs from these in two ways:
%First, it considers not only the cost of maintaining direct syntactic dependencies, but extends this to any statistical relations between words that can be exploited for prediction.
%Second, it weights these relations by the amount of predictive information shared between the words.

Our measure of memory complexity---i.e., the memory entropy $H_M$---straightforwardly generalizes measures based on counting items stored in memory. If each item stored in memory requires $k$ bits of storage, then storing $n$ items would require a capacity of $nk$ bits in terms of memory entropy $H_M$. In general, if memory entropy is $H_M$ and all items stored in memory take $k$ bits each to store, then we can store $H_M/k$ items. 

The memory entropy $H_M$ generalizes this logic, however. The memory entropy $H_M$ can be interpreted as the number of bits of memory capacity required to store $n$ different objects, while allowing that different items stored in memory might take different numbers of bits to store, and also that the memory representation might be able to compress the representations of multiple items when they are stored together, so that the capacity required to store two items might be less than the sum of the capacity required to store each individual item. 

\paragraph{The Dependency Locality Theory}
The connection with the Dependency Locality Theory is particularly interesting.
Our lower bound on memory usage, described in Theorem~\ref{prop:suboptimal} Eq.~\ref{eq:memory-bound}, is formally similar to Storage Cost in the Dependency Locality Theory (DLT) \citep{gibson-linguistic-1998,gibson2000dependency}.
In that theory, storage cost at a given timestep is defined as the \emph{number of predictions} that are held in memory.
Our bound on memory usage is stated in terms of mutual information, which indicates the \emph{amount of predictive information} extracted from the previous context and stored in memory.
Therefore, our measure generalizes DLT storage cost.
Our measure and goes beyond DLT storage cost in that the DLT quantity only considers predictions that are certain, and each prediction is postulated to take an equal amount of memory.
In contrast, our measure of memory usage can be seen as weighting predictions by their certainty and amount of predictive information. 
In this sense, DLT storage cost can be seen as an approximation to Eq.~\ref{eq:memory-bound}.

The other component of the DLT is integration cost, the amount of difficulty incurred by establishing a long-term syntactic dependency. 
In our framework, DLT integration cost corresponds to surprisal given an imperfect memory representation, following \cite{futrell2020lossy}.

There is one remaining missing link between our theory of processing difficulty and theories such as the Dependency Locality Theory:
our information locality theorem says that \emph{statistical} dependencies should be short-term, whereas psycholinguistic theories of locality have typically focused on the time-span of \key{syntactic dependencies}: words which depend on each other to determine the meaning or the well-formedness of a sentence. Statistical dependencies, in contrast, mean that whenever one element of a sequence determines or predicts another element \emph{in any way}, those two elements should be close to each other in time. 

If statistical dependencies, as measured using mutual information, can be identified with syntactic dependencies, then that would mean that information locality is straightforwardly a generalization of dependency locality. \citet{futrell2019syntactic} give theoretical and empirical arguments that this is so. They show that syntactic dependencies as annotated in dependency treebanks identify word pairs with especially high mutual information, and give a derivation showing that this is to be expected according to a formalization of the postulates of dependency grammar. The connection between mutual information and syntactic dependency has also been explored by \citet{}. % maximizing mutual information principle; de paiva alves; yuret


\paragraph{Cue-Based Retrieval Models}

In some psycholinguistic theories, memory-related difficulty arises not because of a bound on memory capacity, but rather because of difficulties involved in retrieving information from memory \citep{}. We are able to prove an analogous theorem that applies to such theories: see Supplementary Material Section TODO for an extension of our analysis to the case involving a short-term memory (STM) with unbounded capacity, a working memory (WM) with limited capacity, and cost associated with communication between WM and STM. Essentially, the constraint on the the memory state in our theorem above can be re-interpreted as a constraint on the capacity of a communication channel linking STM to WM. In particular, this result constrains average surprisal for memory models based on cue-based retrieval such as the ACT-R model of \citet{lewis-activation-based-2005}.

The ACT-R model of \cite{lewis-activation-based-2005} does not have an explicit surprisal cost.
Instead, surprisal effects are interpreted as arising because, in less constraining contexts, the parser is more likely to make decisions that then turn out to be incorrect, leading to additional correcting steps.
%We see this as an algorithmic-level implementation of the justification for surprisal theory provided by \citet{levy2008expectation}:
We view this as an algorithmic-level implementation of a surprisal cost:
If a word $w_t$ is unexpected given the current state of the working memory---i.e., buffers and control states---then their current state must provide insufficient information to constrain the actual syntactic state of the sentence, meaning that the parsing steps made to integrate $w_t$ are likely to include more backtracking and correction steps.
Thus, we argue that cue-based retrieval models predict that the surprisal $- \log P(w_t|m_t)$ will be part of the cost of processing word $w_t$.

Work within cue-based retrieval frameworks has suggested that working memory is not characterized by a decay in information over time, but rather an accumulation of interference among similar items stored in memory \citep[][p. 408]{lewis-activation-based-2005}.
In contrast, the formula for memory usage in Eq.~\ref{eq:memory-bound} might appear to suggest that boundedness of memory entails that representations have to decay over time.
However, this is not the case:
our theorem does not imply that a listener forgets words beyond some amount of time $T$ in the past. 
An optimal listener may well decide to remember information about words more distant than $T$, but in order to stay within the bounds of memory, she can only do so at the cost of forgetting some information about words closer than $T$.
The Information Locality Lower Bound still holds, in the sense that the long-term dependency will cause processing difficulty, even if the long-term dependency is not itself forgotten.
See SI Appendix Section X for a mathematical example illustrating this phenomenon.
y and non-redundantly depends on the character $T$ steps in the past, with $T$ large, this will create a memory cost proportional to $T$.

% TODO: Should we mention retrieval interference effects? If there is any slowdown that is *directly* attributable to similarity-based interference, then I think we can't capture it. But a lossily-compressed memory will have interference, in the sense that more "semantically" similar items in memory are more likely to be confused for each other.

\paragraph{The role of surprisal}

There are more general reasons to believe that any realistic theory of sentence processing must include surprisal as at least a \emph{component} of the cost of processing a word, even if it is not explicitly stated as such. 
%More formally, we argue that processing difficulty for a word $w_t$ must at least be given by $k (-\log P(w_t |m_t)) + R$, with $k$ a scaling factor giving the relative contribution of surprisal to processing cost, and $R$ representing all other factors. The scaling factor $k$ may be small, but it cannot be zero, for both empirical and theoretical reasons. 
There are both empirical and theoretical grounds for this claim.
Empirically, surprisal makes a well-documented and robust contribution to processing difficulty in empirical studies of reading times and event-related potentials \citep{smith2013effect,frank2016erp}. 
Theoretically, surprisal may represent an irreducible thermodynamic cost incurred by any information processing system \citep{landauer,still2012thermodynamic,zenon2019information}, and there are multiple converging theoretical arguments for why it should hold as a cost in human language processing in particular \cite[see][for a review]{levy2013memory}. 


\paragraph{Other models predicting locality effects}

Our information locality result holds that language is easiest to process when elements that depend on each other are close to each other. While this kind of prediction is most closely associated with processing models based on memory limitations, there also exist 

% TODO do we have anything to say about these?

Kimball (1973): Principle of Two Sentences

Rambow and Joshi 1994 using TAG

Marcus 1980 deterministic parsing

(Sabrina Gerth, Memory Limitations in Sentence Processing)

\cite{gerth2009unifying}


\cite{wanner1978atn}
\cite{frazier1978sausage}

%\cite{rambow201512}: count words on stack

\cite{boston2012computational}

\cite{just1992capacity}

\cite{marcus1978theory}: deterministic parser with small lookahead



\paragraph{Previous information locality results}


\citet{futrell2020lossy} describe a processing model where listeners make predictions (and incur surprisal) based on lossy memory representations.
In particular, they consider loss models that delete, erase, or replace words in the past.
Within this model, they were able to establish a similar information locality result, by showing that the theoretical processing difficulty increases when words with high \emph{pointwise mutual information} are separated by large distances \citep{futrell-noisy-context-2017,futrell2019information}. Pointwise mutual information is the extent to which a \emph{particular value} predicts another value in a joint probability distribution. For example, if we have words $w_1$ and $w_2$ in a sentence, their pointwise mutual information is:
\begin{equation*}
    \text{pmi}(w_1; w_2) \equiv \log \frac{P(w_2|w_1)}{P(w_2)}.
\end{equation*}
Mutual information, as we defined it in Eq.~\ref{eq:mi}, is the \emph{average} pointwise mutual information over an entire probability distribution.

Our information locality bound theorem differs from this previous result in three ways:
\begin{enumerate}
    \item \citet{futrell2020lossy} required an assumption that incremental memory is subject to decay over time. In contrast, we do not require any assumptions about incremental memory except that it has bounded capacity (or that retrieval operations have bounded capacity; see below).
    \item Our result is a precise bound, whereas the previous result was an approximation based on neglecting higher-order interactions among words.
    \item Our result is about the fall-off of the mutual information between words, \emph{conditional on the intervening words}. The previous result was about the fall-off of \emph{pointwise} mutual information between specific words, without conditioning on the intervening words.
\end{enumerate}

As noted above, this previous work defined information locality in terms of the \emph{unconditional} (pointwise) mutual information between linguistic elements. 
Prior work has studied how unconditional mutual information decays with distance in natural language texts, at the level of orthographic characters \citep{ebeling-entropy-1994,lin-critical-2017} and words \citep{futrell2019syntactic}.

Here we argue that conditional mutual information is more relevant for measuring memory usage than unconditional mutual information. 
While the decay of conditional mutual information provably provides a lower bound on memory entropy, the decay of unconditional mutual information does not.
In the Supplementary Materials Section TODO, we provide an example of a stochastic process where unconditional mutual information does not decay with distance, but memory requirements remain low.


%head-dependent mutual information hypothesis
%In an idealized model \emph{all} predictive information is mediated through direct syntactic dependencies, and each dependency contributed an equal amount of predictive information, our memory bound would indeed become equivalent to such a metric.
%Note that the average number of objects held in memory simultaneously is equivalent to the average time they are stored.

\subsection{Memory--surprisal trade-off in language production}

An analogous memory--surprisal trade-off exists in language production. In this case, the trade-off arises from the minimization of error in production of linguistic sequences. That is, given a \key{competence language} (a target distribution on words given contexts), a speaker tries to produce a \key{performance language} which is as close as possible to the competence language. The performance language operates under memory constraints, so the performance language will diverge from the competence language due to production errors. When a speaker has more incremental memory about what she has already produced, then she is able to produce linguistic sequences with less error, thus reducing the divergence between the performance language and the competence language. The reduction of this competence--performance divergence is formally equivalent to the minimization of average surprisal from Section~\ref{sec:listener-tradeoff}.

% TODO maybe cite some Karl Lashley stuff? Maryellen MacDonald? Shota Momma?

We derive the existence of this trade-off from the following postulates about language production. Let the competence language be represented by a stationary stochastic process, parameterized by a probability distribution $p(w_t | w_{<t})$ giving the conditional probability of any word $w_t$ given an unbounded number of previous words. Our postulates describe a speaker who tries to find a performance language $q(w_t|m_t)$ to match the the competence language using incremental memory representations $m_t$:

\begin{enumerate}
    \item Production Postulate 1 (Incremental memory). At time $t$, the speaker has an incremental \key{memory state} $m_t$ that contains (1) her stored information about previous words that she has produced, and (2) information about her production target. The memory state is given by a \key{memory encoding function} $M$ such that $m_t = M(w_{t-1}, m_{t-1})$.
    
    \item Production Postulate 2 (Production policy). At time $t$, the speaker produces the next word $w_t$ conditional on her memory state by drawing from a probability distribution $q(w_t | m_t)$. We call $q$ the speaker's \key{production policy}.
    
    \item Production Postulate 3 (Minimizing divergence). The production policy $q$ is selected to minimize the KL divergence from the performance language to the target competence language $p(w_t|w_{<t})$. We call this divergence the \key{competence--performance divergence} under the memory encoding function $M$ and the production policy $q$:
    \begin{align}
    \label{eq:comp-perf-div}
    d^q_M &\equiv D_{\text{KL}} [ p(w_t|w_{<t}) || q(w_t|m_t) ] \\
        &= \sum_{w_{\le t}} p(w_{\le t}) \log \frac{p(w_t | w_{<t})}{q(w_t|m_t)}.
    \end{align}
    The production policy is then the solution to the functional minimization problem:
    \begin{equation}
        \mathop{\text{minimize }}_{q(w_t|m_t)} d^q_M.
    \end{equation}
\end{enumerate}

Completing the link with the memory--surprisal trade-off in comprehension, we note that when the production policy $q(w_t|m_t)$ is selected to minimize the competence--performance divergence $d^q_M$, then this divergence becomes equal to the memory distortion $d_M$ discussed above in the context of comprehension costs. Therefore, under these postulates, the Information Locality Bound Theorem will apply in production as well as comprehension. This means that languages that exhibit information locality can be produced with greater accuracy given limited memory resources.

Although the memory--surprisal trade-off is mathematically equivalent between comprehension and production, its psycholinguistic interpretation is different. In the case of language comprehension, the trade-off represents excess processing \emph{difficulty} arising due to memory constraints. In the case of language production, the trade-off represents \emph{production error} arising due to memory constraints. When memory is constrained, then the speaker's productions will diverge from her target language. And as memory is more and more constrained, this divergence will increase more and more. The degree of divergence is measured in the same units as surprisal, hence the formal equivalence between the listener's and speaker's memory--surprisal trade-offs. 



\subsection{Statistical Studies of Language}

Our work opens up a connection between psycholinguistics, linguistic typology, and statistical studies of language. Here, we survey the connections between our work and previous statistical studies.

\paragraph{Statistical Complexity}
Our formalization of listener memory is related to studies of dynamic systems in the physics literature.
Our memory--surprisal curve is closely related to the \emph{predictive information bottleneck} introduced by \citet{still-information-2014} and studied by \citet{marzen-predictive-2016}; in particular, it is a version of the \emph{recursive information bottleneck} \citep[][\S 4]{still-information-2014}. 
In the limit of optimal prediction and minimal surprisal, our formalization of listener memory is equivalent to the notion of \emph{Statistical Complexity} \citep{crutchfield-inferring-1989}.
In the limit $T \rightarrow \infty$, the quantity in Eq.~\ref{eq:memory-bound} is equal to the \emph{excess entropy} \citep{crutchfield-inferring-1989}.
However, the link between memory and information locality provided by our Theorem~\ref{prop:suboptimal} appears to be a novel theoretical contribution.
Relatedly, \citet{sharan-prediction-2016} shows a link between excess entropy and approximability by $n$-th order Markov models, noting that processes with low excess entropy can be approximated well with Markov models of low order.



%\paragraph{Long-range dependencies in text}    % excess entropy
%\cite{debowski-excess-2011} has studied the excess entropy of language across long ranges of text, in particular studying whether it is finite. % compute excess entropy in text
%Our work contrasts with this work in that we are interested in dependencies within sentences.

\paragraph{Memory and Hierarchical Structure; Finiteness of Memory}
Processing nontrivial hierarchical structures typically requires unbounded amounts of memory.
However, crucially, the \emph{average} memory demand for prediction can be finite, if the probability mass assigned to long dependencies is small.
For instance, languages defined by Probabilistic Context Free Grammars (PCFG) always have finite average memory.
The reason is that PCFGs assign low probabilities to long sequences.\footnote{Proposition 2 in \cite{chi-statistical-1999} implies that words drawn from a PCFG have finite expected length. This implies that average memory demands are finite.}

% TODO: Does this only hold for proper PCFGs?



%\paragraph{Center Embeddings}
%\cite{miller-finitary-1963} attributed the unacceptability of multiple center-embedding to memory limitations.
%\cite{gibson-linguistic-1998}
%\paragraph{Other Psycholinguistic Predictions}
% RF: the fact that you would get locality effects given medium WM capacity, but not very high or very low WM capacity, as Bruno Nicenboim found. And maybe some speaker-listener asymmetries. 
%\paragraph{Speakers}
% RF: what matters for the speaker is not I[w_t, w_0 | w_1, …, w_{t-1}], but I[w_t, w_0 | w_1, …, w_{t-1}, G] where G is some representation of the speaker’s goal (like in the van Dijk paper). This changes the interpretation of the mutual information. For the listener, it’s just redundancy. For the speaker, it’s redundancy *conditional on the goal*—which you could interpret as something like conceptual relatedness of linguistic elements. Then the speaker’s pressure is to keep conceptually related things close. 




