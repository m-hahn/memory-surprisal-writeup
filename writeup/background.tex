A wide range of work has argued that information in natural language utterances is ordered in ways that reduce memory effort, by placing elements close together when they depend on each other in some way. Here, we review these arguments from linguistic and cognitive perspectives.

\subsection{Dependency locality and memory constraints in psycholinguistics}

When producing and comprehending language in real time, a language user must keep track of what she has already produced or heard in some kind of incremental memory store, which is subject to resource constraints.
An early example of this idea is \citet{miller-finitary-1963}, who attributed the unacceptability of multiple center embeddings in English to limitations of human working memory.
Concurrent and subsequent work studied how different grammars induce different memory requirements in terms of the number of symbols that must be stored at each point to produce or parse a sentence \citep{yngve1960model,abney1991memory,gibson1991computational,resnik1992left}. 
In psycholinguistic studies, memory constraints typically manifest in the form of processing difficulty associated with long-term dependencies.
For example, at the level of word-by-word online language comprehension, there is observable processing difficulty at moments when it seems that information about a word must be retrieved from working memory. 
This difficulty increases when there is a great deal of time or intervening material between the point when a word is first encountered and the point when it must be retrieved from memory  \citep{gibson1998linguistic,gibson1999memory,gibson2000dependency,mcelree2000sentence,lewis-activation-based-2005,bartek-search-2011,nicenboim2015working,balling2017effects}. 
That is, language comprehension is harder for humans when words which depend on each other for their meaning are separated by many intervening words.
This idea is most prominently associated with the \key{Dependency Locality Theory} of human sentence processing \citep{gibson2000dependency}.

For example, \citet{grodner-consequences-2005} studied word-by-word reading times in a series of sentences such as~(\ref{ex:grodner}) below. 
\eenumsentence{\label{ex:grodner}
\item The \underline{administrator} who the nurse \underline{supervised}\dots\label{ex:grodner1}
\item The \underline{administrator} who the nurse from the clinic \underline{supervised}\dots
\item The \underline{administrator} who the nurse who was from the clinic \underline{supervised}\dots\label{ex:grodner3}
}
In these sentences, the distance between the noun \emph{administrator} and the verb \emph{supervised} is successively increased. \citet{grodner-consequences-2005} found that as this distance increases, there is a concomitant increase in reading time at the verb \emph{supervised} and following words. The hypothesized reason is the following: at the word \emph{supervised}, a comprehender who is trying to compute the meaning of the sentence must integrate a representation of the verb \emph{supervised} with a representation of the noun \emph{administrator}, which is a direct object of the verb. This integration requires retrieving the representation of \emph{administrator} from working memory. If this representation has been in working memory for a long time---for example as in Sentence~\ref{ex:grodner3} as opposed to \ref{ex:grodner1}---then the retrieval operation \jd{not remembering the details: it's specifically the retrieval operation that they say causes difficulty? not the long maintenance of "administrator" while other elements pile up in working memory?} causes difficulty that manifests as increased reading time. There exists a dependency between the words \emph{administrator} and \emph{supervised}, and more excess processing difficulty is incurred the more the two words are separated. 

%The existence of dependency locality effects in human language processing, and their connection with working memory, are well-established. In this work, we will be interested in predicting word and morpheme order using an information-theoretic model of language processing which accounts for locality effects such as these. \jd{why? why does it follow from the existence of dependency locality effects in processing that we should want to predict word and morpheme order, and that we should want to do so using an information-theoretic model? if this isn't the right place for this, i'd just get rid of this paragraph altogether and go straight into 2.2. in fact, reading the first sentence of 2.2., i think its better to go straight into it without this paragraph. perhaps just add the first sentence of this paragraph to the end of the previous paragraph (ideally with citations). but if the point is simply to say "Locality effects such as these fall out of the information-theoretic account we propose", just say that.}

\subsection{Locality and cross-linguistic universals of order}

Because of the documented difficulty associated with long-term dependencies among words, it has been hypothesized that working memory limitations create a pressure for dependency locality in word order. 
That is, words which depend on each other syntactically should be close to each other in linear order.
There is ample evidence from corpus statistics indicating that dependency locality is a real property of word order across many languages \citep{gildea-optimizing-2007,liu2008dependency,gildea-grammars-2010,futrell-large-scale-2015,liu-dependency-2017,temperley-minimizing-2018}. 
\citet{hawkins-performance-1994,hawkins-efficiency-2003} formulates dependency locality as the Principle of Domain Minimization, and has shown that this principle can explain cross-linguistic universals of word order that have been documented by linguistic typologists for decades \citep{greenberg-universals-1963}. 
An example is order alternation in postverbal constituents in English.
While NP objects ordinarily precede PPs (\ref{ex:heavy-np-1}, example from~\citet{staub2006heavy}), this order is less preferred when the NP is very long (\ref{ex:heavy-np-2}), in which case the inverse order becomes more felicitous (\ref{ex:heavy-np-3}).
The pattern in (\ref{ex:heavy-np-3}) is known as Heavy NP Shift \citep{ross1967constraints,arnold2000heaviness,stallings2011s}.
Compared to (\ref{ex:heavy-np-2}), it reduces the distance between the verb ``ate'' and the PP, while only modestly increasing the distance between the verb and object NP.

\eenumsentence{\label{ex:heavy-np}
\item Lucy ate [the broccoli] with a fork.\label{ex:heavy-np-1}
\item Lucy ate [the extremely delicious, bright green broccoli] with a fork .\label{ex:heavy-np-2}
\item Lucy ate with a fork [the extremely delicious, bright green broccoli].\label{ex:heavy-np-3}
}

%TODO dependency locality examples \mhahn{how about a heavy NP shift example?} \rljf{Yes, the ``threw out the trash'' example should owrk here} \mhahn{can use real Heavy NP shify example}

%While dependency locality has a strong ability to predict word order universals, there are certain classes of locality phenomena that are not captured by the theory. For example, ordering asymmetries between arguments and adjuncts remain unexplained \citep{}. TODO \mhahn{what is missing here?}

Locality principles have also appeared in a more general form in the functional linguistics literature, in the form of the idea that elements which are more `relevant' to each other will appear closer to each other in linear order in utterances \citep{behaghel1932deutsche,givon1985iconicity,givon1991markedness,bybee-morphology-1985,newmeyer1992iconicity}. Here, `elements' can refer to words or morphemes, and the definition of `relevance' varies. For example, \citet{givon1985iconicity}'s \key{Proximity Principle} states that elements are placed closer together in a sentence if they are closer conceptually.
Applying a similar principle, \citet{bybee-morphology-1985} studied the order or morphemes within words across languages, and argued that (for example) morphemes that indicate the valence of a verb (whether it takes zero, one, or two objects) are placed closer to the verb root than morphemes that indicate the plurality of the subject of the verb, because the valence morphemes are more `relevant' to the verb root. %In previous literature, the Proximity Principle has been motivated in terms of iconicity; here, we will explain it in terms of the same kinds of online memory limitations that have been used to motivate the principle of dependency locality in syntax.

While these theories are widespread in the linguistics literature, there exists to date no quantifiable definition of `relevance' or `being closer cenceptually'. One of our contributions is to derive such a notion of `relevance' from the minimization of memory usage during language processing. \jd{make sure this relevance point is taken back up in the general discussion}


\subsection{Cognitive Underpinnings}
\jd{change section header to sth like "Cognitive architecture" or "Architectural assumptions" or "Memory architecture"?}

The connection between memory resources and locality principles relies on the idea that limitations in working memory will give rise to difficulty when elements that depend on each other are separated at a large distance in time. In previous work, this idea has been motivated in terms of specific assumptions about the architecture of memory. For example, models of memory in sentence processing differ in whether they assume limitations in storage capacity \citep[e.g.][]{gibson1998linguistic} \jd{here, gibson is portrayed as a capacity theory; above it's portrayed as a retrieval theory} or the precision with which specific elements can be retrieved from memory \citep[e.g.][]{lewis-activation-based-2005}. Furthermore, in order to derive the connection between memory usage in such models and locality in word order, it has been necessary to stipulate that memory representations or activations decay over time in some way to explain why longer dependencies are harder to process.
The question remains of whether these assumptions about memory architecture are necessary, or whether word orders across languages are optimized for memory independently of the implementation and architecture of human language processing.

In this work, we adopt an information-theoretic perspective on working memory\jd{someone might say that the assumption that this is an account of working memory rather than different types of memory is already an architectural assumption} which abstracts away from the details of memory architecture.
%From an information-theoretic perspective, the different ways in which memory can be constrained---constraints on storage capacity, representation precision, etc.---are different instantiations of the same quantity: namely, the mutual information between past input and the representations that are used for processing new material \citep{still-information-2014}. \mhahn{do we need to say more here?}
Within our framework, we will establish the connection between memory resources and locality principles by providing general information-theoretic lower bounds on memory load.
%We will consider a general setting of a listener performing incremental prediction.
Our result immediately entails a link between locality and boundedness of memory, without any stipulations\jd{isn't "without any stipulations" too strong? the main stipulation is that the relevant memory representation is bits of information -- seems like a minor stipulation (and it should be noted somewhere that this subsumes different, more mechanistic assumptions about the nature of memory representations (right?), but a stipulation nonetheless, and one that's easy to acknowledge} about memory representations, and in particular without any assumption that memory representations or activations decay over time \citep[as was required in][]{gibson1998linguistic, lewis-activation-based-2005, futrell2020lossy}.
We will then show empirical evidence that the orders of words and morphemes in natural language are structured in a way that reduces our measure of memory load compared to the orders of counterfactual baseline languages.


The remainder of the paper is structured as follows. In Section 3, we introduce the memory-surprisal tradeoff, show that information locality is a property of languages with more efficient memory-surprisal tradeoffs, and introduce the main hypothesis that emerges from the account: the Efficient Tradeoff Curve Hypothesis. Sections 4, 5, and 6 are dedicated to testing the Efficient Tradeoff Curve Hypothesis. In Section 4, we qualitatively test the Hypothesis in a reanalysis of word orders emerging in a miniature artificial language study \CITE.
In Section 5, we quantitatively test the Hypothesis in a large-scale study of the word order of 54 languages. In Section 6, we test the Hypothesis on morpheme order in Japanese and Sesotho.
Finally, in Section 7 we discuss the implications and limitations of the reported results."


\jd{writing the above, i realized that it would be really useful if there was a way to introduce the ETCH in some informal way towards the very beginning of the paper, in the intro. i know this is probably difficult, but if there's an intuitive way to formulate the prediction (and simply defer the formal definition to section 3), i think that would make reading the background easier -- because we know what we're working towards -- and would also make writing the proposed forward-looking section a little easier to write}

%\mhahn{can also mention \cite{christiansen2016now}}

% Production argument
% Idea: A producer wants to approximate a language conditional on a production target G.
% Ie producer finds m_t to minimize loss: D[ w_t | w_{<t}, g   ||   w_t  | m_t  ] + a * H[m_t]
% That is, the memory has to contain both the previous words and the current production goal.
% The loss comes out to I[ w_t : w_{<t},g | m_t] + a * H[m_t]
%                       = I[w_t : w_{<t} | m_t] + I[w_t : g | w_{<t}, m_t] + a * H[m_t]
% Now let's decompose the memory m_t into two parts:
% the part about the previous words: call this r_t
% the part about the current goal: call this g_t. And let's assume H[g|g_t]=0, i.e. the memory stores
% all the information about the goal.
% Then H[m_t] = H[r_t] + H[g_t|r_t].
% Now we have loss:
%     I[w_t : w_{<t} | r_t, g] + I[w_t : g | w_{<t}, r_t, g] + a H[r_t, g_t]
%   = I[w_t : w_{<t} | r_t, g] + a H[r_t, g_t] 
% Now we have H[r_t, g_t] >= H[r_t] >= \sum t I_t.
% And then we have 


