A wide range of work has argued that information in natural language utterances is ordered in ways that reduce memory effort, by placing elements close together when they depend on each other in some way. Here, we review these arguments from linguistic and cognitive perspectives.

\subsection{Dependency locality and memory constraints in psycholinguistics}

When producing and comprehending language in real time, a language user must keep track of what she has already produced or heard in some kind of incremental memory store, which is subject to resource constraints.
An early example of this idea is \citet{miller-finitary-1963}, who attributed the unacceptability of multiple center embeddings in English to limitations of human working memory.
Concurrent and subsequent work studied how different grammars induce different memory requirements in terms of the number of symbols that must be stored at each point to produce or parse a sentence \citep{yngve1960model,abney1991memory,gibson1991computational,resnik1992left}. 
In psycholinguistic studies, memory constraints typically manifest in the form of processing difficulty associated with long-term dependencies.
For example, at the level of word-by-word online language comprehension, there is observable processing difficulty at moments when it seems that information about a word must be retrieved from working memory. 
This difficulty increases when there is a great deal of time or intervening material between the point when a word is first encountered and the point when it must be retrieved from memory  \citep{gibson1998linguistic,gibson1999memory,gibson2000dependency,mcelree2000sentence,lewis-activation-based-2005,bartek-search-2011,nicenboim2015working,balling2017effects}. 
That is, language comprehension is harder for humans when words which depend on each other for their meaning are separated by many intervening words.
This idea is most prominently associated with the \key{Dependency Locality Theory} of human sentence processing \citep{gibson2000dependency}.

For example, \citet{grodner-consequences-2005} studied word-by-word reading times in a series of sentences such as~(\ref{ex:grodner}) below. 
\eenumsentence{\label{ex:grodner}
\item The \underline{administrator} who the nurse \underline{supervised}\dots\label{ex:grodner1}
\item The \underline{administrator} who the nurse from the clinic \underline{supervised}\dots
\item The \underline{administrator} who the nurse who was from the clinic \underline{supervised}\dots\label{ex:grodner3}
}
In these sentences, the distance between the noun \emph{administrator} and the verb \emph{supervised} is successively increased. \citet{grodner-consequences-2005} found that as this distance increases, there is a concomitant increase in reading time at the verb \emph{supervised} and following words. 

The hypothesized reason for this reading time pattern is based on memory constraints. The idea goes: at the word \emph{supervised}, a comprehender who is trying to compute the meaning of the sentence must integrate a representation of the verb \emph{supervised} with a representation of the noun \emph{administrator}, which is a direct object of the verb. This integration requires retrieving the representation of \emph{administrator} from working memory. If this representation has been in working memory for a long time---for example as in Sentence~\ref{ex:grodner3} as opposed to \ref{ex:grodner1}---then the retrieval is difficult or inaccurate, in a way that manifests as increased reading time. Essentially, there exists a dependency between the words \emph{administrator} and \emph{supervised}, and more excess processing difficulty is incurred the more the two words are separated; this excess difficulty is called a \key{dependency locality effect}.

The existence of dependency locality effects in human language processing, and their connection with working memory, are well-established \citep{fedorenko2013direct}. These locality effects in online processing mirror locality effects in word order, described below.



\subsection{Locality and cross-linguistic universals of order}

Dependency locality in word order means that there is a pressure for words which depend on each other syntactically to be close to each other in linear order.
There is ample evidence from corpus statistics indicating that dependency locality is a real property of word order across many languages \citep{gildea-optimizing-2007,liu2008dependency,gildea-grammars-2010,futrell-large-scale-2015,liu-dependency-2017,temperley-minimizing-2018}. 
\citet{hawkins1994performance,hawkins-efficiency-2003} formulates dependency locality as the Principle of Domain Minimization, and has shown that this principle can explain cross-linguistic universals of word order that have been documented by linguistic typologists for decades \citep{greenberg-universals-1963}. 
Such as pressure can be motivated in terms of the documented online processing difficulty associated with long-term dependencies among words: dependency locality in word order means that online processing is easier.

An example is order alternation in postverbal constituents in English.
While NP objects ordinarily precede PPs (\ref{ex:heavy-np-1}, example from~\citet{staub2006heavy}), this order is less preferred when the NP is very long (\ref{ex:heavy-np-2}), in which case the inverse order becomes more felicitous (\ref{ex:heavy-np-3}).
The pattern in (\ref{ex:heavy-np-3}) is known as Heavy NP Shift \citep{ross1967constraints,arnold2000heaviness,stallings2011s}.
Compared to (\ref{ex:heavy-np-2}), it reduces the distance between the verb ``ate'' and the PP, while only modestly increasing the distance between the verb and object NP.

\eenumsentence{\label{ex:heavy-np}
\item Lucy ate [the broccoli] with a fork.\label{ex:heavy-np-1}
\item ? Lucy ate with a fork [the broccoli].\label{ex:heavy-np-bad}
\item Lucy ate [the extremely delicious, bright green broccoli] with a fork .\label{ex:heavy-np-2}
\item Lucy ate with a fork [the extremely delicious, bright green broccoli].\label{ex:heavy-np-3}
}

Locality principles have also appeared in a more general form in the functional linguistics literature, in the form of the idea that elements which are more `relevant' to each other will appear closer to each other in linear order in utterances \citep{behaghel1932deutsche,givon1985iconicity,givon1991markedness,bybee-morphology-1985,newmeyer1992iconicity}. Here, `elements' can refer to words or morphemes, and the definition of `relevance' varies. For example, \citet{givon1985iconicity}'s \key{Proximity Principle} states that elements are placed closer together in a sentence if they are closer conceptually.
Applying a similar principle, \citet{bybee-morphology-1985} studied the order or morphemes within words across languages, and argued that (for example) morphemes that indicate the valence of a verb (whether it takes zero, one, or two objects) are placed closer to the verb root than morphemes that indicate the plurality of the subject of the verb, because the valence morphemes are more `relevant' to the verb root. %In previous literature, the Proximity Principle has been motivated in terms of iconicity; here, we will explain it in terms of the same kinds of online memory limitations that have been used to motivate the principle of dependency locality in syntax.

While these theories are widespread in the linguistics literature, there exists to date no quantifiable definition of `relevance' or `being closer cenceptually'. One of our contributions is to derive such a notion of `relevance' from the minimization of memory usage during language processing.


\subsection{Architectural assumptions}

The connection between memory resources and locality principles relies on the idea that limitations in working memory will give rise to difficulty when elements that depend on each other are separated at a large distance in time. In previous work, this idea has been motivated in terms of specific assumptions about the architecture of memory. For example, models of memory in sentence processing differ in whether they assume limitations in storage capacity \citep[e.g., `memory cost' in the model of][]{gibson1998linguistic} or the precision with which specific elements can be retrieved from memory \citep[e.g.][]{lewis-activation-based-2005}. Furthermore, in order to derive the connection between memory usage in such models and locality in word order, it has been necessary to stipulate that memory representations or activations decay over time in some way to explain why longer dependencies are harder to process.
The question remains of whether these assumptions about memory architecture are necessary, or whether word orders across languages are optimized for memory independently of the implementation and architecture of human language processing.

In this work, we adopt an information-theoretic perspective on memory use in language processing, which abstracts away from the details of memory architecture. 
Within our framework, we will establish the connection between memory resources and locality principles by providing general information-theoretic lower bounds on memory use.
Our result immediately entails a link between locality and boundedness of memory, following only from the stipulation that memory has bounded capacity measured in bits. 
In particular, our model does not require any assumption that memory representations or activations decay over time \citep[as was required in][]{gibson1998linguistic, lewis-activation-based-2005, futrell2020lossy}.
We will then show empirical evidence that the orders of words and morphemes in natural language are structured in a way that reduces our measure of memory use compared to the orders of counterfactual baseline languages.

The remainder of the paper is structured as follows. In Section 3, we introduce the memory-surprisal tradeoff, show that information locality is a property of languages with more efficient memory-surprisal tradeoffs, and introduce the main hypothesis that emerges from the account: the Efficient Tradeoff Hypothesis. Sections 4, 5, and 6 are dedicated to testing the Efficient Tradeoff Hypothesis. In Section 4, we qualitatively test the Hypothesis in a reanalysis of word orders emerging in a miniature artificial language study \citep{fedzechkina-human-2017}.
In Section 5, we quantitatively test the Hypothesis in a large-scale study of the word order of 54 languages. In Section 6, we test the Hypothesis on morpheme order in Japanese and Sesotho.
In Section 7 we discuss the implications and limitations of the reported results.
Section 8 concludes.


%\jd{writing the above, i realized that it would be really useful if there was a way to introduce the ETCH in some informal way towards the very beginning of the paper, in the intro. i know this is probably difficult, but if there's an intuitive way to formulate the prediction (and simply defer the formal definition to section 3), i think that would make reading the background easier -- because we know what we're working towards -- and would also make writing the proposed forward-looking section a little easier to write}

%\mhahn{can also mention \cite{christiansen2016now}}

% Production argument
% Idea: A producer wants to approximate a language conditional on a production target G.
% Ie producer finds m_t to minimize loss: D[ w_t | w_{<t}, g   ||   w_t  | m_t  ] + a * H[m_t]
% That is, the memory has to contain both the previous words and the current production goal.
% The loss comes out to I[ w_t : w_{<t},g | m_t] + a * H[m_t]
%                       = I[w_t : w_{<t} | m_t] + I[w_t : g | w_{<t}, m_t] + a * H[m_t]
% Now let's decompose the memory m_t into two parts:
% the part about the previous words: call this r_t
% the part about the current goal: call this g_t. And let's assume H[g|g_t]=0, i.e. the memory stores
% all the information about the goal.
% Then H[m_t] = H[r_t] + H[g_t|r_t].
% Now we have loss:
%     I[w_t : w_{<t} | r_t, g] + I[w_t : g | w_{<t}, r_t, g] + a H[r_t, g_t]
%   = I[w_t : w_{<t} | r_t, g] + a H[r_t, g_t] 
% Now we have H[r_t, g_t] >= H[r_t] >= \sum t I_t.
% And then we have 


