%%
%% This is file `./samples/longsample.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% apa7.dtx  (with options: `longsample')
%% ----------------------------------------------------------------------
%% 
%% apa7 - A LaTeX class for formatting documents in compliance with the
%% American Psychological Association's Publication Manual, 7th edition
%% 
%% Copyright (C) 2019 by Daniel A. Weiss <daniel.weiss.led at gmail.com>
%% 
%% This work may be distributed and/or modified under the
%% conditions of the LaTeX Project Public License (LPPL), either
%% version 1.3c of this license or (at your option) any later
%% version.  The latest version of this license is in the file:
%% 
%% http://www.latex-project.org/lppl.txt
%% 
%% Users may freely modify these files without permission, as long as the
%% copyright line and this statement are maintained intact.
%% 
%% This work is not endorsed by, affiliated with, or probably even known
%% by, the American Psychological Association.
%% 
%% ----------------------------------------------------------------------
%% 
\documentclass[man]{apa7}

\usepackage{lipsum}

\usepackage[american]{babel}


\usepackage{amsmath}
\usepackage{lingmacros}

\usepackage{tikz-dependency}

\usepackage{changepage}

\usepackage{linguex}


\usepackage{csquotes}
\usepackage[bibencoding=utf8, style=apa,sortcites=true,sorting=nyt,backend=biber]{biblatex}
%\usepackage[backend=biber, bibencoding=utf8, style=authoryear, citestyle=authoryear]{biblatex} % https://tex.stackexchange.com/questions/402714/biblatex-not-working-with-biber-2-8-error-reading-bib-file-in-ascii-format
\DeclareLanguageMapping{american}{american-apa}
\addbibresource{literature.bib}



\title{Modeling Word and Morpheme Order in Natural Language as an Efficient Tradeoff of Memory and Surprisal}
\shorttitle{Tradeoff of Memory and Surprisal}

\author{Michael Hahn$^1$, Judith Degen$^1$, Richard Futrell$^2$}
\affiliation{$^1$ Stanford University, $^2$ University of California, Irvine}

\leftheader{Hahn, Degen, Futrell}


%\newcommand\citep[1]{\parencite{#1}}
%\newcommand\citet[1]{\Textcite{#1}}

\newcommand{\citep}{\parencite}
\newcommand{\citet}{\Textcite}

\definecolor{Purple}{RGB}{255,10,140}
\newcommand{\jd}[1]{\textcolor{Purple}{[jd: #1]}} 
\newcommand\rljf[1]{\textcolor{blue}{[rljf: #1]}}



\newcommand{\soft}[1]{}
\newcommand{\nopreview}[1]{}
\newcommand\comment[1]{{\color{red}#1}}
\newcommand\mhahn[1]{{\color{red}(mhahn #1)}}
\newcommand\jdegen[1]{{\color{red}(jdegen #1)}}
\newcommand\REF[0]{{\color{red}(REF) }}
\newcommand\CITE[0]{{\color{red}(CITE) }}
\newcommand{\key}[1]{\textbf{#1}}


%\usepackage[dvipsnames]{xcolor}

\usepackage{amsthm}

\newcommand{\thetad}[0]{{\theta_d}}
\newcommand{\thetal}[0]{{\theta_{LM}}}

\newcounter{theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{thm}[theorem]{Theorem}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{question}[theorem]{Question}
\newtheorem{example}[theorem]{Example}
\newtheorem{lemma}[theorem]{Lemma}

\abstract{
Memory has well-documented effects on language at different time scales. For instance, memory limitations are known to constrain language comprehension and production. They have also been argued to account for crosslinguistic word order regularities.
However, a systematic assessment of the role of memory limitations in the structure of language has proven elusive, in part because it is hard to extract precise large-scale quantitative generalizations about language from the existing mechanistic models of memory use in sentence processing.
Here, we provide an architecture-independent information-theoretic formalization of memory limitations which enables a simple calculation of the memory efficiency of languages.
Our notion of memory efficiency is based on the idea of a \emph{memory--surprisal tradeoff}: that a certain level of average surprisal per word can only be achieved at the cost of storing some amount of information about past context.
Based on the idea of this notion of memory usage, we advance the \emph{Efficient Tradeoff Hypothesis}: that the order of elements in natural language is under a pressure to enable favorable memory--surprisal tradeoffs.
We derive that languages enable more efficient tradeoffs when they exhibit \emph{information locality}: when predictive information about an element (like a word) is concentrated in its recent past. 
Next, we provide empirical evidence from three test domains in support of the Efficient Tradeoff Hypothesis: 
a reanalysis of a miniature artificial language learning experiment, a large-scale study of word order in corpora of 54 languages, and an analysis of morpheme order in two agglutinative languages. These results suggest that principles of order in natural language can be explained via highly generic cognitively motivated principles and lend support to efficiency-based models of the structure of human language.}

\keywords{Language universals, sentence processing, information theory}

\authornote{
   \addORCIDlink{Michael Hahn}{0000-0003-4828-4834}

  Correspondence concerning this article should be addressed to Michael Hahn, Department of Linguistics, Stanford University Stanford, CA 94305-2150.  E-mail: mhahn2@stanford.edu}
  
  % Old abstract
  
  %Memory has well-documented effects on language at different time scales. For instance, memory limitations are known to constrain language comprehension and production. They have also been argued to account for crosslinguistic word order regularities. However, a systematic assessment of the role of memory limitations in the structure of language has proven elusive, partly due to the architecture-dependence of many memory accounts of language. \jd{not sure about the previous sentence -- basically, we need to be clear what the motivation is for the work. if someone has a better idea, please insert.} Building on expectation-based models of language processing, we provide an architecture-independent information-theoretic formalization of memory limitations. We show that comprehenders can achieve lower average surprisal per word at the cost of storing more information about past context, an idea we refer to as the \emph{memory--surprisal tradeoff}. We show that the shape of the tradeoff is determined in part by the order of linguistic elements, e.g., word order or morpheme order. In particular, languages enable more efficient tradeoffs when they exhibit \emph{information locality}: if predictive information about an element (like a word) is concentrated in its recent past. The account yields a novel prediction, which we term the \emph{Efficient Tradeoff Hypothesis}: that the order of elements in natural language is characterized by a distinctively steeper memory-surprisal tradeoff curve compared to other possible orders. We provide evidence from three test domains in support of the Efficient Tradeoff Hypothesis: a reanalysis of a miniature artificial grammar experiment shows that languages resulting from regularization exhibit a steeper tradeoff curve than the original input language; evidence from corpora of 54  languages shows that the grammar of real languages allows for more efficient memory--surprisal tradeoffs than random baseline word order grammars; and an analysis of two agglutinative languages shows that optimization of the memory-surprisal tradeoff is also reflected in morpheme order. These results suggest that principles of order in natural language can be explained via highly generic cognitively motivated principles

\begin{document}

\maketitle
\end{document}

%\appendix
