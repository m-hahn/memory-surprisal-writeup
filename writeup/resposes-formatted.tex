\documentclass{article}[11pt,a4paper,oneside]
\usepackage[utf8]{inputenc}
\usepackage{fullpage}

\usepackage{linguex}

\title{Response to Editor and Reviewers}
%\date{February 2019}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{url}


\usepackage{changepage}   % for the adjustwidth environment

\usepackage{xcolor}
\newcommand\mhahn[1]{\textcolor{red}{[mhahn: #1]}}
\newcommand\rljf[1]{\textcolor{green}{[rljf: #1]}}
\newcommand\action[1]{\textcolor{blue}{Action Item: #1}}
\newcommand\meeting[1]{\textcolor{orange}{Meeting notes: #1}}
%\newcommand\response[1]{\textcolor{blue}{#1}}

\newcommand\response[1]{\begin{adjustwidth}{1.0cm}{0.0cm}\textcolor{blue}{#1}\end{adjustwidth}}

\usepackage[utf8]{inputenc}
\usepackage{color}
\usepackage{framed}
\usepackage{graphicx}

%\renewcommand{\closing}[1]{\par\nobreak\vspace{\parskip}%
%\stopbreaks
%\noindent
%\parbox{\indentedwidth}{\raggedright
%    \ignorespaces #1\\[0.0in]%
%    \ifx\@empty\fromsig
%        \fromname
%\else \fromsig \fi\strut}%
%\par}

\newcommand{\key}[1]{\textbf{#1}}

\newenvironment{point}
   {\bigskip \noindent --- }
   {\par }

\newenvironment{reply}
  {\par\medskip
   \color{blue}%
   \begin{framed}
   \textbf{Reply: }\ignorespaces}
 {\end{framed}
  \medskip}   



\begin{document}

\maketitle

We thank the reviewers for their insightful comments; we have considered them carefully in revising the paper. In the following, we respond to the comments, describing how we revised the paper to address the concerns and suggestions.



\section{Reviewer 1}



My biggest concerns are regarding Figures 10 and 11. Page 19 claims, "In most languages, $I_1$ is distinctly larger for the actual and fitted orderings compared to the baseline orderings." But this doesn't match Figure 10, where the red line (baseline) is consistently above the green (fitted), and in most cases very close to blue (real). I'm wondering if the legend is wrong/the graph is miscolored, and green line is in fact the baseline condition? I find it hard to make sense of both this graph and the ensuing results otherwise.

\begin{reply}
Apologies, this was a typo, which we've now fixed.
\end{reply}

Figures 10 through 12 are missing axis labels. I'm particularly concerned about how to interpret the x axis in Figure 11. Is this indicating bits of memory? If so, it seems like in many cases, all of the interesting differentiation is occuring at less than 1 bit of memory. This seems like a bizarrely small amount of memory to be concerning ourselves with.

\begin{reply}
We have added the missing axis labels. \mhahn{TODO do this}
The x-axis indicates nats of memory, i.e. computing logarithms using base e.
	\mhahn{We might change it to bits, though changing it throughout the entire paper \& SI seems like a bigger task}

	While 1 bit (or nat) of memory seems like a small difference, this is a difference in cost at \emph{every word}, which accumulates over a sentence.
	In a sentence with 20 words, the overall number of bits that have to be encoded over time (though not simultaneously) additionally might add up to 20 bits.
	\mhahn{do we have to say something in the paper?}
\end{reply}

While the UD corpora are the largest annotated cross-linguistic corpora readily available, they are in fact quite small (as indicated by the fact that the average training data size was ~5,000 sentences per language). This means that the neural models from which the $I_t$ are estimated are probably not very predictive except of extremely high frequency words/collocations. How would this bias in terms of what can be correctly predicted affect the memory-surprisal tradeoff results?

\begin{reply}
	\mhahn{TODO should we add this discussion in General Discussion?}
We expect that small data size will result in underestimating $I_t$, in particular as $t$ increases. This may bias estimates towards curves where surprisal doesn't further decrease beyond a certain level of memory.
However, as this equally applies to difefrent ordering grammars, we expect that estimating the relative efficiency of different orderings at the same level of memory is still reliable.
\end{reply}

Minor concerns:
Page 17 states, "Some languages enable more efficient tradeoffs than others by allowing a listener to store fewer bits in memory to achieve the same level of average surprisal." From the remainder of the paper, it's clear that this is refering to some theoretically possible languages, rather than some real languages, but it would be helpful to clarify that the goal of this paper is not to attempt to rank real languages in terms of which are more efficient than others.

\begin{reply}
	TODO do this
\end{reply}

In Figure 9, I believe Grammar 1 should have subjects = 0.2 and objects = -0.8.

\begin{reply}
	TODO do this
\end{reply}


\section{Reviewer 2}

- Having $m_t$ not include the t'th word I found a little confusing. I think it would be more natural to think about e.g. $P(w_t | m_{t-1})$

\begin{reply}
	Our choice of writing $P(w_t | m_{t})$ follows the convention for HMMs as in, e.g. \mhahn{cite something}.
	We also feel that this choice helps reduce the number of times ``+1'' or ``-1'' occurs in formulas, making them a bit easier to read.
\end{reply}

- "The claim that processing difficulty should be directly proportional to surprisal comes from surprisal theory " -- I think citation/discussion of Smith \& Levy is probably good here (they have two different derivations of why surprisal should matter to RT)

\begin{reply}
	We have added: ``\textit{and there are several converging theoretical arguments for surprisal as a measure of processing cost (Levy, 2008; Smith \& Levy, 2013).}''
\end{reply}

- I wonder if there is a simpler way to think about the theorem if you took time out of it -- this isn't quite it, but if you just said there was some memory capacity M (which is informative about the outcome / next word), some entropy rate R (you called it $S_\infty$), and some information I don't have in memory called W (so that the total memory+context/environment has M+W bits). If S is my personal entropy rate (given M), and I was encoding/predicting efficiently, I think you'd have S = R+W-M (my uncertainty is at best the true uncertainty plus uncertainty from stuff I don't model minus my knowledge), which implies S > R+W. Something like that? Not sure if that's quite right, but it seems like it's much simpler to think about without time, and you might consider a time-free version first to help give intuitions)

\begin{reply}
The argment described by the reviewer can be formalized as
\begin{equation}
H[X_{t}|M_t] \geq H[X_{t}] - H[M_t]
\end{equation}
where the term on the left corresponds to $S$, and the term on the right corresponds to $R+W-M$.
This inequality follows from the Data Processing Inequality.

This can be restated as 
\begin{equation}
H[M_t] \geq H[X_{t}] - H[X_{t}|M_t]
\end{equation}
i.e., memory load is lower-bounded by the surprisal reduction compared to the memory-free setting.

	Given that $H[X_{t}] - H[X_t|X_{t}] = \sum_{t=1}^\infty I_t$, this entails that, for any $T$, a listener investing at most $H[M_t] \leq \sum_{t=1}^T I_t$ bits of memory incurs average surprisal at least $H[X_t|X_{<t}] + \sum_{t=T+1}^\infty I_t$.

This looks very similar to the theorem, but the crucial difference is that the factor $t$ is missing in the term bounding bits of memory, i.e., this bound is in general weaker than the one provided by the theorem.
	We believe that taking in time is necessary to derive the stronger bound $\sum_{t=1}^T t I_t$.
\end{reply}

- I wondered about the assumption that carrying I bits for t timesteps should cost me I*t (e.g. pg 18 "The minimal amount of memory capacity which would be required to retain this information is the sum $\sum t I_t$"). I don't understand why that's not just $\sum I_t$. I found this pretty confusing and couldn't tell if I just should be ignoring the t and thinking of like (11) as just giving me a bound as a function of T. That is, do we need to interpret the "t" inside the sum, or are we just treating $\sum_{t=1}^T t I_t$ as an arbitrary mathematical function that defines T?

\begin{reply}
An intuition for the factor $t$ is that, if $I_t$ bits of information have to be carried over $t$ timesteps on average, then these bits occupy memory for $t$ timesteps.
This means that, for any given point in time, there are $t$ ``packages'' with an average size of $I_t$ bits each that have to be carried across it, summing up to $t I_t$.

We have added this intuitive argument directly below the statement of the theorem.
\end{reply}

- It wasn't clear to me what the cost of remembering is (is that where the "t" in the previous point comes from?). It could be, for instance, that remembering information is essentially free and only reading/writing is costly (this is, for instance, how a thumb drive works). There's always an opportunity cost of other things you could have stored, but there's a huge literature (that I don't know well) in memory research about the relative costs of encoding, decoding, interference, and maintenance, which seems like it should be brought in here. What happens, for instance, if there are primarily costs associated with retrieval? Or interference in memory?

\begin{reply}
We provide discussion of retrieval and interference in the subsection on `Cue-Based Retrieval Models' in the General Discussion and in SI Section 1.3, where we show that a version of the information locality theorem continues to hold in a model that assumes that maintenance is free and that the cost is entirely in retrieval operations.
	We have made these results more prominent in the paper by making the following changes \mhahn{TODO}.
\end{reply}

- One thing that always puzzled me about DLT was that, if I recall correctly, it only counted nouns in its dependency length. I believe it was because nouns were supposed to interfere with each other more. I wonder what this theory has to say about that, and maybe more generally what the right unit of "time" is (words? syllables? seconds? nouns?)

\begin{reply}
In the memory-surprisal tradeoff, intervening elements come in via the definition of $I_t$, which controls for redundancy with intervening material.
Which concrete consequences for nouns and other intervening elements this has will depend on their co-occurrence statistics.
	\mhahn{what should we say here?}


Regarding the unit of time, the memory-surprisal tradeoff should hold at all units of time at which humans engage in incremental predictive processing.
	\mhahn{what should we say here?}

\end{reply}

- Why it's $\sum t I_t$ was really not clear to me when I got to (11), until later. It might help to introduce that early.

\begin{reply}
	TODO do this
\end{reply}

- Note sure why Fedzechkina is always cited with a first name.

\begin{reply}
	We fixed this. \mhahn{TODO fix this}
\end{reply}

- Study 2 seems much stronger to me and I think should be presented before Study 1. They also seem like separable questions -- Study 2 is about what happened in natural languages, whereas Study 1 looks at what happens in a lab setting within individuals (which is a less natural "experiment")

\begin{reply}
We chose to keep the order, as we believe Study 1 can serve as an illustration of the theoretical results in a controlled but language-like setting.
\end{reply}

- I would suggest moving some methods of Study 2 into the SI.

\begin{reply}
	TODO think about this
\end{reply}

- For the "four exceptions" in Figure 11 -- it's worth discussing maybe whether this method works less well on some languages (depending on their dependency corpora, etc.)?

\begin{reply}
	TODO think about this
\end{reply}

- The morphology section made me wonder if there was an analogous analysis one could apply to e.g. adjective ordering in English

\begin{reply}
We have added a discussion of the difference to dependency locality in \mhahn{where?}, where we cite studies providing evidence of information locality in adjective ordering.
\end{reply}

- I wondered throughout if entropy of memory $H_M$ was really the right measure, as opposed to e.g. mutual information between memory and upcoming material. While these quantities are related (and may be equal in an idealized picture the authors have in mind), entropy could just be true disorder and thus not informative about the upcoming material. This makes me think the entropy part of memory is less important to emphasize than the informativeness part.

\begin{reply}
We have added a paragraph at the end of SI Section 1.2, explaining how the theoretical result remains true when modeling memory cost as the mutual information $I_M$ between memory and preceding material.
	We have also added a pointer to this in the main paper in \mhahn{where?}.

The information between memory and *preceding* material is an upper bound on the information between memory and *upcoming* material suggested by the reviewer.
We believe the information with *preceding* material is more appropriate because it accounts for the fact that the comprehender may not know which aspects of the past input ultimately are useful for predicting the future, so might have to store some bits that turn out to be ultimately useless. This is captured both by the entropy measure and the mutual information with preceding material, though not by mutual information with upcoming material.
\end{reply}

- For the discussion on entropy in memory, the authors might find it informative to look at e.g. Konkle, Brady, Alvarez and those kinds of models -- they cite work by Landauer etc which tries to estimate the information capacity of memory (under various assumptions).

\begin{reply}
	TODO maybe cite these more prominently?
\end{reply}

- Is it possible to disentangle predictions from this paper's theory from minimum dependency length theories? Are they always the same or are there some technical differences? If they are ever different, it would be nice to see how and where exactly, even if they just predict for instance different patterns of noise around the optimum.


\begin{reply}
	\mhahn{We should point out that info locality makes predictions beyond DLM, in morpheme ordering and adjective ordering. Where do we do this in the paper?}
\end{reply}



\section{Reviewer 3}

The paper provides empirical evidence for the efficient memory-surprisal trade-off hypothesis in the domains of word and morpheme order. The findings are novel and I think the paper will be of interest to the wide audience of Psychological Review.

I only have a few smaller comments.

1. I would like to push the authors to be a bit more clear and consistent about the source of the memory-surprisal trade-off. On p. 3, they present the model as a comprehension model, on p. 10, they state that the trade-off applies to comprehension and production, while on p. 44, they state that they leave the mechanism open (whether it's production, comprehension, or acquisition).

\begin{reply}
	TODO emphasize the comprehension model more \mhahn{We could remove the paragraph on Production on page 10 to streamline this a bit.}
\end{reply}

2. I missed the discussion of connections and implications of the current findings to typology in the general discussion (despite the promise of them on p. 43). I'm curious about what phenomena the memory-surprisal trade-off can capture and where its predictions might differ from other theories.

\begin{reply}
	TODO think about this
\end{reply}

3. I'm a bit confused by the results pertaining to the artificial grammar. The background on p.20 led me to believe that what is going to be predicted is sentence word order (A or B). However, the results reference the findings about the case marker, which wasn't even mentioned in the background. I think its relevance to the prediction and its function in the artificial grammar need to be explained in more detail.

\begin{reply}
	TODO make this clearer \mhahn{how to make this clearer?}
\end{reply}

4. Finally, what I missed about the memory-surprisal trade-off in the paper is why I as researcher should investigate it further and what it provides that previous models do not. Does it capture the artificial/natural language data better than dependency locality? Is it a more parsimonious model? Does it capture a wider range of phenomena? Where do its predictions diverge from the previous models? I'd like to see some of such discussion in the paper.

\begin{reply}
	We have added discussion of the differences with dependency locality in \mhahn{where?}.
\end{reply}



\end{document}
