\documentclass[11pt,letterpaper]{article}

\usepackage{showlabels}
\usepackage{fullpage}
\usepackage{pslatex}
%\usepackage{latexsym}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{url}
%\usepackage[colorinlistoftodos]{todonotes}
\usepackage{rotating}
\usepackage{natbib}
\usepackage{amssymb}
\usepackage{lingmacros}

\usepackage{tikz-dependency}
\usepackage{longtable}

\usepackage{changepage}


%\usepackage{paralist} 
%\usepackage{graphicx} 
%\usepackage{multirow} 
%\usepackage{enumitem}
\usepackage{linguex}
%\raggedbottom

\definecolor{Purple}{RGB}{255,10,140}
\newcommand{\jd}[1]{\textcolor{Purple}{[jd: #1]}} 


\newcommand{\R}[0]{\mathbb{R}}
\newcommand{\E}[0]{\mathbb{E}}
\newcommand{\Ff}[0]{\mathcal{F}}

\usepackage{multirow}

\newcommand{\soft}[1]{}
\newcommand{\nopreview}[1]{}
\newcommand\comment[1]{{\color{red}#1}}
\newcommand\mhahn[1]{{\color{red}(mhahn #1)}}
\newcommand\jdegen[1]{{\color{red}(jdegen #1)}}
\newcommand\note[1]{{\color{red}(#1)}}
\newcommand\REF[0]{{\color{red}(REF) }}
\newcommand\CITE[0]{{\color{red}(CITE) }}
\newcommand\rljf[1]{{\color{red}(#1)}}
\newcommand{\key}[1]{\textbf{#1}}


%\usepackage[dvipsnames]{xcolor}

\usepackage{amsthm}

\newcommand{\thetad}[0]{{\theta_d}}
\newcommand{\thetal}[0]{{\theta_{LM}}}

\newcounter{theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{thm}[theorem]{Theorem}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{question}[theorem]{Question}
\newtheorem{example}[theorem]{Example}
\newtheorem{lemma}[theorem]{Lemma}


\frenchspacing
%\def\baselinestretch{0.975}

%\emnlpfinalcopy
%\def\emnlppaperid{496}

\title{Modeling word and morpheme order in natural language as an efficient tradeoff of memory and surprisal}
\author{Michael Hahn, Judith Degen, Richard Futrell}
%\date{2018}

\begin{document}

\maketitle


\begin{abstract}
Memory has well-documented effects on language at different time scales. For instance, memory limitations are known to constrain language comprehension and production. They have also been argued to account for crosslinguistic word order regularities. However, a systematic assessment of the role of memory limitations in the structure of language has proven elusive, partly due to the architecture-dependence of many memory accounts of language. \jd{not sure about the previous sentence -- basically, we need to be clear what the motivation is for the work. if someone has a better idea, please insert.} Building on expectation-based models of language processing, we provide an architecture-independent information-theoretic formalization of memory limitations. We show that comprehenders can achieve lower average surprisal per word at the cost of storing more information about past context, an idea we refer to as the \emph{memory--surprisal tradeoff}. We show that the shape of the tradeoff is determined in part by the order of linguistic elements, e.g., word order or morpheme order. In particular, languages enable more efficient tradeoffs when they exhibit \emph{information locality}: if predictive information about an element (like a word) is concentrated in its recent past. The account yields a novel prediction, which we term the \emph{Efficient Tradeoff Hypothesis}: that the order of elements in natural language is characterized by a distinctively steeper memory-surprisal tradeoff curve compared to other possible orders. We provide evidence from three test domains in support of the Efficient Tradeoff Hypothesis: a reanalysis of a miniature artificial grammar experiment shows that languages resulting from regularization exhibit a steeper tradeoff curve than the original input language; evidence from corpora of 54  languages shows that the grammar of real languages allows for more efficient memory--surprisal tradeoffs than random baseline word order grammars; and an analysis of two agglutinative languages shows that optimization of the memory-surprisal tradeoff is also reflected in morpheme order. These results suggest that principles of order in natural language can be explained via highly generic cognitively motivated information-theoretic principles \jd{as opposed to what? be specific about the contribution this makes to the Big Questions people care about}.
\end{abstract}

\jd{2 stylistic notes on plots: 1. it would generally be better to use a color-blind friendly palette. 2. it would be helpful if a particular color had a particular semantics throughout the paper, eg, if real languages across plots always had the same color, if different baselines that correspond to analogous versions of each other across plots did, etc. not strictly necessary, just makes it a lot easier on the reader.}
 
\section{Introduction}
\input{introduction.tex}


\section{Background}\label{sec:background}


\input{background.tex}


\section{Memory-Surprisal Tradeoff}\label{sec:ms-tradeoff}

\input{tradeoff.tex}

\section{Study 1: Memory and Dependency Length}

\input{toy-experiment.tex}



\section{Study 2: Large-Scale Evidence that Word Orders Optimize Memory-Surprisal Tradeoff}
\label{sec:main-experiment}

\input{main-experiment.tex}

\subsection{Controlling for Information Structure}\label{subsec:freedom}

\input{freedom-control.tex}

\section{Study 3: Morpheme Order}

\input{word-internal.tex}


\section{General Discussion}

\input{discussion.tex}


\section{Conclusion}

In this work, we have provided evidence that human languages order elements in a way that reduces cognitive resource requirements, in particular memory effort.
We provided an information-theoretic formalization of memory requirements as a tradeoff of memory and surprisal.
We show theoretically that languages have more efficient tradeoffs when they show stronger degrees of information locality.
Information locality provides a formalization of various locality principles from the linguistic literature, including dependency locality \citep{gibson1998linguistic}, domain minimization \citep{hawkins2004efficiency}, and the proximity principle \citep{givon1985iconicity}.
Using this result, we provided evidence that languages order words and morphemes in such a way as to provide efficient memory--surprisal tradeoffs.

Our result shows that wide-ranging principles of order in natural language can be explained from highly generic cognitively-motivated information-theoretic principles. The locality properties we have discussed are some of the most characteristic properties of natural language, setting natural language apart from other codes studied in information theory.
Therefore, our result raises the question of whether other distinctive characteristics of language---for example, mildly context-sensitive syntax, duality of patterning, and compositionality---might also be explained in terms of information-theoretic resource constraints on production and comprehension.



\bibliographystyle{apalike}
\bibliography{literature}

\end{document}






