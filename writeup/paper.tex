\documentclass[11pt,letterpaper]{article}

\usepackage{showlabels}
\usepackage{fullpage}
\usepackage{pslatex}
%\usepackage{latexsym}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{url}
%\usepackage[colorinlistoftodos]{todonotes}
\usepackage{rotating}
\usepackage{natbib}
\usepackage{amssymb}
\usepackage{lingmacros}

\usepackage{tikz-dependency}
\usepackage{longtable}

\usepackage{changepage}


%\usepackage{paralist} 
%\usepackage{graphicx} 
%\usepackage{multirow} 
%\usepackage{enumitem}
\usepackage{linguex}
%\raggedbottom

\definecolor{Purple}{RGB}{255,10,140}
\newcommand{\jd}[1]{\textcolor{Purple}{[jd: #1]}} 


\newcommand{\R}[0]{\mathbb{R}}
\newcommand{\E}[0]{\mathbb{E}}
\newcommand{\Ff}[0]{\mathcal{F}}

\usepackage{multirow}

\newcommand{\soft}[1]{}
\newcommand{\nopreview}[1]{}
\newcommand\comment[1]{{\color{red}#1}}
\newcommand\mhahn[1]{{\color{red}(mhahn #1)}}
\newcommand\jdegen[1]{{\color{red}(jdegen #1)}}
\newcommand\note[1]{{\color{red}(#1)}}
\newcommand\REF[0]{{\color{red}(REF) }}
\newcommand\CITE[0]{{\color{red}(CITE) }}
\newcommand\rljf[1]{{\color{red}(#1)}}
\newcommand{\key}[1]{\textbf{#1}}


%\usepackage[dvipsnames]{xcolor}

\usepackage{amsthm}

\newcommand{\thetad}[0]{{\theta_d}}
\newcommand{\thetal}[0]{{\theta_{LM}}}

\newcounter{theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{thm}[theorem]{Theorem}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{question}[theorem]{Question}
\newtheorem{example}[theorem]{Example}
\newtheorem{lemma}[theorem]{Lemma}


\frenchspacing
%\def\baselinestretch{0.975}

%\emnlpfinalcopy
%\def\emnlppaperid{496}

\title{Modeling word and morpheme order in natural language as an efficient tradeoff of memory and surprisal}
\author{Michael Hahn, Judith Degen, Richard Futrell}
%\date{2018}

\begin{document}

\maketitle


\begin{abstract}
Online memory limitations are well-established as a factor impacting sentence processing and have been argued to account for crosslinguistic word order regularities. Building off expectation-based models of language processing, we provide an information-theoretic formalization of these memory limitations. We introduce the idea of a memory--surprisal tradeoff: comprehenders can achieve lower average surprisal per word at the cost of storing more information about past context. We show that the shape of the tradeoff is determined in part by word order. In particular, languages will enable more efficient tradeoffs when they exhibit information locality: when predictive information about a word is concentrated in the wordâ€™s recent past. We show evidence from corpora of 54 real languages showing that languages allow for more efficient memory--surprisal tradeoffs than random baseline word order grammars. 
We then use data from two agglutinative languages to show that morpheme order also optimizes efficiency of this tradeoff.
\end{abstract}


 
 \jd{at the very end, make sure to either consistently use past or present tense for things that were done (i prefer past tense for reports of what was done, and present tense for conclusions, implications, generalizations)}
 
\section{Introduction}
\input{introduction.tex}


\section{Background}\label{sec:background}


\input{background.tex}


\section{Memory-Surprisal Tradeoff}\label{sec:ms-tradeoff}

\input{tradeoff.tex}

\section{Study 1: Memory and Dependency Length}

\input{toy-experiment.tex}



\section{Study 2: Large-Scale Evidence that Word Orders Optimize Memory-Surprisal Tradeoff}
\label{sec:main-experiment}

\input{main-experiment.tex}

\subsection{Controlling for Information Structure}\label{subsec:freedom}

\input{freedom-control.tex}

\section{Study 3: Morpheme Order}

\input{word-internal.tex}


\section{General Discussion}

\input{discussion.tex}


\section{Conclusion}

In this work, we have provided evidence that human languages order elements in a way that reduces cognitive resource requirements, in particular memory effort.
We provided an information-theoretic formalization of memory requirements as a tradeoff of memory and surprisal.
We show theoretically that languages have more efficient tradeoffs when they show stronger degrees of information locality.
Information locality provides a formalization of various locality principles from the linguistic literature, including dependency locality \citep{gibson1998linguistic}, domain minimization \citep{hawkins2004efficiency}, and the proximity principle \citep{givon1985iconicity}.
Using this result, we provided evidence that languages order words and morphemes in such a way as to provide efficient memory--surprisal tradeoffs.

Our result shows that wide-ranging principles of order in natural language can be explained from highly generic cognitively-motivated information-theoretic principles. The locality properties we have discussed are some of the most characteristic properties of natural language, setting natural language apart from other codes studied in information theory.
Therefore, our result raises the question of whether other distinctive characteristics of language---for example, mildly context-sensitive syntax, duality of patterning, and compositionality---might also be explained in terms of information-theoretic resource constraints on production and comprehension.



\bibliographystyle{apalike}
\bibliography{literature}

\end{document}






